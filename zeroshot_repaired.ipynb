{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ZeroShot (Algo 1)\n",
    "\n",
    "    for total_batches in range(total_n_pseudo_batches):\n",
    "    \n",
    "        z = tf.random.normal([batch_size, z_dim])\n",
    "        pseudo_images = get_gen_images(z)\n",
    "        teacher_logits, *teacher_activations = get_model_outputs(teacher_model, pseudo_images, mode=0)\n",
    "\n",
    "        #generator training\n",
    "        for ng in range(ng_batches):\n",
    "            student_logits, *student_activations = get_model_outputs(student_model, pseudo_images, mode=1)\n",
    "            generator_loss = generator_loss(teacher_logits, student_logits)\n",
    "\n",
    "            #################################\n",
    "            # BACK PROP AND tick schedulers #\n",
    "            #################################  \n",
    "\n",
    "        for ns in range(ns_batches):\n",
    "            student_logits, *student_activations = get_model_outputs(student_model, pseudo_images, mode=1)\n",
    "            student_loss = student_loss(teacher_logits, teacher_activations, \n",
    "                                        student_logits, student_activations, attn_beta)\n",
    "\n",
    "            #################################\n",
    "            # BACK PROP AND tick schedulers #\n",
    "            #################################   \n",
    "\n",
    "        ######################################################\n",
    "        ### Val accuracy computation and best model saving ###\n",
    "        ######################################################    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.losses import KLDivergence\n",
    "from tensorflow.keras.utils import normalize\n",
    "\n",
    "def kd_loss(p_true, p_pred):\n",
    "    \"\"\"\n",
    "    Kullback Leibler divergence loss\n",
    "    Args:\n",
    "        p_true:\n",
    "        p_pred:\n",
    "    Ref:\n",
    "    https://www.tensorflow.org/versions/r1.14/api_docs/python/tf/keras/losses/KLDivergence\n",
    "    \"\"\"\n",
    "    return KLDivergence()(p_true, p_pred)\n",
    "\n",
    "def generator_loss_fn(t_logits, s_logits, temp=1):\n",
    "\n",
    "\tloss = kd_loss(\n",
    "            tf.math.softmax(t_logits / temp) ,\n",
    "            tf.math.softmax(s_logits / temp))\n",
    "\n",
    "\tg_loss = -loss\n",
    "\n",
    "\treturn g_loss\n",
    "\n",
    "\n",
    "def student_loss_fn(t_logits, t_acts, s_logits, s_acts, beta, temp=1):\n",
    "    \"\"\"\n",
    "    The student loss function used in\n",
    "        - zero-shot learning\n",
    "        - Knowledge-distillaton with attention term (KD-AT)\n",
    "        - few-shot learning, few samples as KD-AT\n",
    "    See Section 3.2, Eq. 1\n",
    "    Args:\n",
    "        t_logits: Teacher logits\n",
    "        t_acts:  list of teacher activation layer output\n",
    "        s_logits: Student logits\n",
    "        s_acts: list of student activation layer output\n",
    "        beta: hyper-parameter for tuning the weight of the attention term\n",
    "    Return:\n",
    "        loss\n",
    "    \"\"\"\n",
    "    loss = kd_loss(\n",
    "            tf.math.softmax(t_logits / temp) ,\n",
    "            tf.math.softmax(s_logits / temp))\n",
    "\n",
    "    if beta != 0.0:\n",
    "        att_loss = 0.0\n",
    "        for t_act, s_act in zip(t_acts, s_acts):\n",
    "            att_loss += attention_loss(t_act, s_act)\n",
    "\n",
    "        loss += beta * att_loss\n",
    "\n",
    "    return loss\n",
    "\n",
    "def __spatial_attention_map(act_tensor, p=2):\n",
    "    \"\"\"\n",
    "    Spatial attention mapping function to map the activation tensor with shape\n",
    "    (H, W, C) to (H, W).\n",
    "    We employed:\n",
    "        sum of absolute values raised to the power of 2\n",
    "    The f(A_{l}) is the paper of replication\n",
    "    Args:\n",
    "        act_tensor: activation tensor with shape (H, W, C)\n",
    "    Return:\n",
    "        a spatial attention map with shape (H, W)\n",
    "    Migration:\n",
    "        corr. to f_act\n",
    "    \"\"\"\n",
    "\n",
    "    out = tf.pow(act_tensor, p)\n",
    "    out = tf.reduce_mean(out, axis=-1)\n",
    "    # flatten it\n",
    "    out = tf.reshape(out, [out.shape[0], -1])\n",
    "\n",
    "    # renormalize them\n",
    "    out = tf.linalg.l2_normalize(out)\n",
    "    return out\n",
    "\n",
    "def attention_loss(act1, act2):\n",
    "    \"\"\"\n",
    "    Return the activation loss. The loss is the L2 distances between two\n",
    "    activation map\n",
    "    Args:\n",
    "        act_map_1:\n",
    "        act_map_2:\n",
    "    Return:\n",
    "        a floating point number representing the loss. As we use tensorflow,\n",
    "        the floating point number would be a number hold in tf.Tensor\n",
    "    Bug:\n",
    "        1. use this with beta = 250 will blow up the err and the grad will explode\n",
    "        2. their implementation not using beta = 250\n",
    "    Ref:\n",
    "    https://github.com/szagoruyko/attention-transfer/blob/893df5488f93691799f082a70e2521a9dc2ddf2d/utils.py#L22\n",
    "    \"\"\"\n",
    "    # get the activation map first\n",
    "    act_map_1 = __spatial_attention_map(act1)\n",
    "    act_map_2 = __spatial_attention_map(act2)\n",
    "\n",
    "    if False: # paper impl.\n",
    "        # calculate vector norm of vectorized matrix\n",
    "        out = tf.pow(act_map_1 - act_map_2, 2)\n",
    "        out = tf.reduce_sum(out)\n",
    "        ret = tf.sqrt(out)\n",
    "    else:\n",
    "        # their code impl\n",
    "        out = tf.pow(act_map_1 - act_map_2, 2)\n",
    "        out = tf.reduce_mean(out)\n",
    "        ret = out\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"wide-resnet-16-2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 32, 32, 16)   432         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 32, 32, 16)   64          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 32, 32, 16)   0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 32)   4608        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 32)   128         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 32)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 32)   512         activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 32)   9216        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 32, 32, 32)   0           conv2d_3[0][0]                   \n",
      "                                                                 conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 32)   128         add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 32)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 32)   9216        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 32)   128         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 32)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 32)   9216        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32, 32, 32)   0           add[0][0]                        \n",
      "                                                                 conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 32)   128         add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 32)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 16, 16, 64)   18432       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 16, 16, 64)   256         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 16, 16, 64)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 16, 64)   2048        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 16, 16, 64)   36864       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 16, 16, 64)   0           conv2d_8[0][0]                   \n",
      "                                                                 conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 16, 16, 64)   256         add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 16, 16, 64)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 16, 64)   36864       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 16, 16, 64)   256         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16, 16, 64)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 64)   36864       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 16, 16, 64)   0           add_2[0][0]                      \n",
      "                                                                 conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 16, 16, 64)   256         add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 16, 16, 64)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 8, 8, 128)    73728       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 8, 8, 128)    512         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 8, 8, 128)    0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 8, 8, 128)    8192        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 8, 8, 128)    147456      activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 8, 8, 128)    0           conv2d_13[0][0]                  \n",
      "                                                                 conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 8, 8, 128)    512         add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 8, 8, 128)    0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 8, 8, 128)    147456      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 8, 8, 128)    512         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 8, 8, 128)    0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 8, 8, 128)    147456      activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 8, 8, 128)    0           add_4[0][0]                      \n",
      "                                                                 conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 8, 8, 128)    512         add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 8, 8, 128)    0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 1, 1, 128)    0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 128)          0           average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "logits (Dense)                  (None, 10)           1290        flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "softmax (Softmax)               (None, 10)           0           logits[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 693,498\n",
      "Trainable params: 691,674\n",
      "Non-trainable params: 1,824\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Wide Residual Network models for Keras.\n",
    "This implementation follows the implementation of the authors' lua version.\n",
    "Missing info if only check with the author paper:\n",
    "    1. Add batchnorm + relu before the avg_pool layer\n",
    "    2. conv1x1 used to adjust the input output size.\n",
    "       it is called down sampling in the paper\n",
    "       no bn+relu before this down sample conv1x1\n",
    "Reference:\n",
    "    - [Wide Residual Networks](https://arxiv.org/abs/1605.07146)\n",
    "    - https://towardsdatascience.com/review-wrns-wide-residual-networks-image-classification-d3feb3fb2004\n",
    "    - https://github.com/szagoruyko/wide-residual-networks\n",
    "    - https://github.com/szagoruyko/wide-residual-networks/blob/master/models/wide-resnet.lua\n",
    "    - https://github.com/xternalz/WideResNet-pytorch/blob/master/wideresnet.py\n",
    "Notes:\n",
    "    1. Used Pre-Activation ResNet\n",
    "        performing batch norm and ReLU before convolution\n",
    "        i.e. BN-ReLU-Conv\n",
    "TODO:\n",
    "    2. model.add_lost\n",
    "Visualize the network:\n",
    "    ```\n",
    "    $ python wide_resnet.py\n",
    "    ```\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Add\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Softmax\n",
    "from tensorflow.keras.layers import Lambda\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "\n",
    "class Identity(Layer):\n",
    "    \"\"\"\n",
    "    Identity layer, like nn.Identity in pytorch\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Identity, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, x):\n",
    "        return x\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "def WideResidualNetwork(depth=28, width=8, dropout_rate=0.0,\n",
    "                        input_shape=None, classes=10, weight_decay=0.0,\n",
    "                        has_softmax=True, output_activations=False):\n",
    "    \"\"\"\n",
    "    Builder function to make wide-residual network\n",
    "    \"\"\"\n",
    "    # --------------------------------------------------------------------------\n",
    "    # input args checking\n",
    "    if (has_softmax and output_activations):\n",
    "        # FIXME: Fix the wordings\n",
    "        raise ValueError(\"we should not need both softmax and activations at the same time.\")\n",
    "\n",
    "    if (depth - 4) % 6 != 0:\n",
    "        raise ValueError('Depth of the network must be such that (depth - 4)'\n",
    "                         'should be divisible by 6.')\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # make model name\n",
    "    model_name = 'wide-resnet-{}-{}'.format(depth, width)\n",
    "\n",
    "\n",
    "    img_input = Input(shape=input_shape)\n",
    "\n",
    "    ret = __create_wide_residual_network(classes, img_input,\n",
    "            depth=depth,\n",
    "            width=width,\n",
    "            dropout=dropout_rate,\n",
    "            weight_decay=weight_decay,\n",
    "            has_softmax=has_softmax,\n",
    "            output_activations=output_activations,\n",
    "            model_name=model_name)\n",
    "\n",
    "\n",
    "    return ret\n",
    "\n",
    "def __conv1_block(input_, weight_decay=0.0):\n",
    "    \"\"\"\n",
    "    The first convolution layer of WRN.\n",
    "    As the paper call it conv1 group, we call it conv1_block for convention\n",
    "    \"\"\"\n",
    "\n",
    "    x = Conv2D(16, kernel_size=3, padding='same', use_bias=False,\n",
    "               kernel_regularizer=regularizers.l2(weight_decay))(input_)\n",
    "    return x\n",
    "\n",
    "\n",
    "def __basic_residual_basic_block(input_, nInputPlane, nOutputPlane, strides, dropout=0.0, weight_decay=0.0):\n",
    "    \"\"\"\n",
    "    See [Wide Residual Networks] Figure 1(a); B(3, 3) implementation\n",
    "    TODO:\n",
    "        doc\n",
    "    \"\"\"\n",
    "\n",
    "    # ==================\n",
    "    # residual blocks\n",
    "    # ==================\n",
    "    # Pre-Activation\n",
    "    x = BatchNormalization()(input_)\n",
    "    x = Activation('relu')(x)\n",
    "    short_circuit_start = x  # mark this block as the short_circuit_start point\n",
    "    x = Conv2D(nOutputPlane, kernel_size=3, strides=strides, padding='same',\n",
    "               use_bias=False, kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    # Mentioned in the paper section 2.4\n",
    "    # A dropout layer shoud be after ReLU to perturb batch normalization in the\n",
    "    # next residual block and prevent it from overfitting.\n",
    "    if dropout > 0:\n",
    "        x = Dropout(dropout)(x)\n",
    "\n",
    "    x = Conv2D(nOutputPlane, kernel_size=3, strides=1, padding=\"same\",\n",
    "               use_bias=False, kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
    "\n",
    "    # ==================\n",
    "    # short circuit\n",
    "    # ==================\n",
    "    if nInputPlane != nOutputPlane:\n",
    "        init = Conv2D(nOutputPlane, kernel_size=1, strides=strides,\n",
    "                      use_bias=False, kernel_regularizer=regularizers.l2(weight_decay))(short_circuit_start)\n",
    "    else:\n",
    "        init = input_\n",
    "\n",
    "    m = Add()([init, x])\n",
    "    return m\n",
    "\n",
    "def __residual_block_group(input_, nInputPlane, nOutputPlane, count, strides, dropout=0.0, weight_decay=0.0):\n",
    "    \"\"\"\n",
    "    For stacking blocks\n",
    "    TODO:\n",
    "        doc\n",
    "    \"\"\"\n",
    "    x = input_\n",
    "    for i in range(count):\n",
    "        if i == 0:\n",
    "            x = __basic_residual_basic_block(\n",
    "                    x, nInputPlane, nOutputPlane, strides, dropout=dropout, weight_decay=weight_decay)\n",
    "        else:\n",
    "            # As the first block in group resolved unequal input and output\n",
    "            # on this block, the strides will be 1 for this\n",
    "            x = __basic_residual_basic_block(\n",
    "                x, nOutputPlane, nOutputPlane, strides=1, dropout=dropout, weight_decay=weight_decay)\n",
    "    return x\n",
    "\n",
    "\n",
    "def __create_wide_residual_network(nb_classes, img_input, depth=28,\n",
    "                                   width=8, dropout=0.0,\n",
    "                                   weight_decay=0.0,\n",
    "                                   has_softmax=True,\n",
    "                                   output_activations=False, model_name=None):\n",
    "    ''' Creates a Wide Residual Network with specified parameters\n",
    "    Args:\n",
    "        nb_classes: Number of output classes\n",
    "        img_input: Input layer\n",
    "        depth: Depth of the network. Compute N = (n - 4) / 6.\n",
    "               For a depth of 16, n = 16, N = (16 - 4) / 6 = 2\n",
    "               For a depth of 28, n = 28, N = (28 - 4) / 6 = 4\n",
    "               For a depth of 40, n = 40, N = (40 - 4) / 6 = 6\n",
    "        width: Width of the network.\n",
    "        dropout: Adds dropout if value is greater than 0.0\n",
    "    Returns:\n",
    "        a Keras Model\n",
    "    Notes:\n",
    "        N is a number of blocks in group.\n",
    "        minus 4 as we have\n",
    "            1. 1 conv3x3 in conv1_block group\n",
    "            2. 1 conv in each group for upsample / downsample in shortcut\n",
    "               Each group has exactly one conv1x1 as shortcut size tunning\n",
    "    '''\n",
    "    N = (depth - 4) // 6\n",
    "\n",
    "    x = __conv1_block(img_input, weight_decay=weight_decay)\n",
    "\n",
    "    nChannels = [16, 16*width, 32*width, 64*width]\n",
    "\n",
    "    # Block Group: conv2\n",
    "    x = __residual_block_group(x, nChannels[0], nChannels[1],\n",
    "                               count=N, strides=1, dropout=dropout,\n",
    "                               weight_decay=weight_decay)\n",
    "    act1 = x\n",
    "    # att1 = Identity(name='attention1')(x)  # Identity layer\n",
    "\n",
    "\n",
    "    # Block Group: conv3\n",
    "    x = __residual_block_group(x, nChannels[1], nChannels[2],\n",
    "                               count=N, strides=2, dropout=dropout,\n",
    "                               weight_decay=weight_decay)\n",
    "    act2 = x\n",
    "    # att2 = Identity(name='attention2')(x)  # Identity layer\n",
    "\n",
    "    # Block Group: conv4\n",
    "    x = __residual_block_group(x, nChannels[2], nChannels[3],\n",
    "                               count=N, strides=2, dropout=dropout,\n",
    "                               weight_decay=weight_decay)\n",
    "    act3 = x\n",
    "    # att3 = Identity(name='attention3')(x)  # Identity layer\n",
    "\n",
    "\n",
    "    # Avg pooling + fully connected layer\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)    # relu is a must add otherwise cannot train\n",
    "    x = AveragePooling2D((8, 8))(x)\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    # Final classification layer\n",
    "    x = Dense(nb_classes, name='logits')(x)\n",
    "    if has_softmax and not output_activations:\n",
    "        x = Softmax(axis=-1)(x)\n",
    "\n",
    "    # make model as the return\n",
    "    if output_activations:\n",
    "        ret = Model(inputs=img_input, outputs=[x, act1, act2, act3], name=model_name)\n",
    "    else:\n",
    "        ret = Model(inputs=img_input, outputs=x, name=model_name)\n",
    "\n",
    "    return ret\n",
    "\n",
    "# ========================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pass\n",
    "    n = 16\n",
    "    k = 2\n",
    "    model = WideResidualNetwork(n, k, input_shape=(32, 32, 3), dropout_rate=0.0, weight_decay=0.002)\n",
    "    model.summary()\n",
    "    # model.load_weights('test.h5')\n",
    "    # # plt_name = \"WRN-{}-{}.pdf\".format(n, k)\n",
    "    # # from tensorflow.keras.utils import plot_model\n",
    "    # # plot_model(model, plt_name, show_shapes=True, show_layer_names=True)\n",
    "    # # model.save_weights('test.h5')\n",
    "\n",
    "    # # model2 = WideResidualNetwork(n, k, input_shape=(32, 32, 3), dropout_rate=0.0, output_activations=True)\n",
    "    # # # from tensorflow.keras.utils import plot_model\n",
    "    # # # plt_name = \"new-WRN-{}-{}.pdf\".format(n, k)\n",
    "    # # # plot_model(model, plt_name, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The generator use to provide a training sample that teacher and students has\n",
    "the most difference estimation distribution.\n",
    "TODO:\n",
    "    Understand how this structure comes\n",
    "Quote:\n",
    "    We use a generic generator with only three convolutional layers,\n",
    "    and our input noise z has 100 dimensions\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "# from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.layers import Reshape\n",
    "\n",
    "\n",
    "def NavieGenerator(input_dim=100):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Dense(8 * 8 * 128, input_shape=(input_dim,)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "\n",
    "    model.add(Reshape((8, 8, 128)))\n",
    "    assert model.output_shape == (None, 8, 8, 128)\n",
    "\n",
    "    model.add(Conv2DTranspose(128, (3, 3), strides=(2, 2), padding='same'))\n",
    "    assert model.output_shape == (None, 16, 16, 128)\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "\n",
    "    model.add(Conv2DTranspose(64, (3, 3), strides=(2, 2), padding='same'))\n",
    "    assert model.output_shape == (None, 32, 32, 64)\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "\n",
    "    model.add(Conv2DTranspose(3, (3, 3), strides=(1, 1), padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    assert model.output_shape == (None, 32, 32, 3)\n",
    "    return model\n",
    "\n",
    "# function alias\n",
    "generator = NavieGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#from net.generator import NavieGenerator\n",
    "#####from utils.cosine_anealing import CosineAnnealingScheduler\n",
    "#####from utils.losses import student_loss_fn, generator_loss_fn\n",
    "#from utils.losses import student_loss_fn\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "#from net.wide_resnet import WideResidualNetwork\n",
    "from tensorflow.keras.experimental import CosineDecay\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dim = 100\n",
    "batch_size = 128\n",
    "ng_batches = 1\n",
    "ns_batches = 10\n",
    "attn_beta = 250\n",
    "total_n_pseudo_batches = 80000\n",
    "student_lr = 1e-3\n",
    "generator_lr = 2e-3\n",
    "\n",
    "teacher = WideResidualNetwork(40, 2, input_shape=(32, 32, 3), dropout_rate=0.0, has_softmax=False, output_activations=True)\n",
    "#teacher.load_weights('exp1_wrn-16-1-seed23_cifar10_WRN-16-1-seed23_model.171.h5')\n",
    "teacher.load_weights('cifar10_WRN-40-2-seed23_model.186.h5')\n",
    "teacher.trainable = False\n",
    "\n",
    "student = WideResidualNetwork(16, 1, input_shape=(32, 32, 3), dropout_rate=0.0, has_softmax=False, output_activations=True)\n",
    "student_optimizer = Adam(learning_rate=CosineDecay(student_lr, total_n_pseudo_batches*ns_batches))\n",
    "\n",
    "generator = NavieGenerator(input_dim=100)\n",
    "generator_optimizer = Adam(learning_rate=CosineDecay(generator_lr, total_n_pseudo_batches))\n",
    "\n",
    "\n",
    "# Generator loss metrics\n",
    "g_loss_met = tf.keras.metrics.Mean()\n",
    "# Student loss metrics\n",
    "stu_loss_met = tf.keras.metrics.Mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: generator mean loss = -2.282476\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "step 0-9: studnt mean loss = 1.0006514\n",
      "step 1: generator mean loss = -0.030416649\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "step 1-9: studnt mean loss = 0.30817515\n",
      "step 2: generator mean loss = -0.17294392\n",
      "[2 2 2 3 2 3 2 2 2 2 2 2 2 2 2 3 2 2 2 2 3 2 3 2 3 2 2 2 3 2 2 2 2 2 2 2 2\n",
      " 2 2 3 2 2 3 2 2 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2 2 2 3 2 2 2 3 2 2\n",
      " 2 2 2 3 2 2 2 3 3 2 2 3 2 2 3 2 2 2 2 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2 2 3]\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "step 2-9: studnt mean loss = 9.632488\n",
      "step 3: generator mean loss = -9.238077\n",
      "[3 3 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3\n",
      " 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "step 3-9: studnt mean loss = 3.0614452\n",
      "step 4: generator mean loss = -3.2060957\n",
      "[3 3 3 3 3 3 3 8 3 8 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 3 3 8 3 8 3 3 3 3 3\n",
      " 8 3 3 3 3 8 3 3 8 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 3 3 8 3 3 3 8 3 3 3 3\n",
      " 3 3 3 3 3 8 3 3 3 3 3 3 3 3 3 3 3 3 3 3 8 3 3 3 3 8 8 3 3 3 3 3 3 3 3 3 3\n",
      " 3 8 3 3 3 8 3 3 3 8 3 3 3 3 8 3 3]\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "step 4-9: studnt mean loss = 3.8781276\n",
      "step 5: generator mean loss = -2.8374429\n",
      "[2 2 8 3 3 8 8 2 2 8 3 8 8 2 2 8 2 8 3 2 2 2 8 2 3 2 2 8 3 2 3 8 8 2 8 2 8\n",
      " 2 8 8 3 3 8 8 8 8 2 8 3 8 2 8 8 2 8 3 8 2 3 8 8 2 3 3 2 8 2 3 8 2 2 2 8 3\n",
      " 3 8 3 8 8 8 2 3 8 2 8 2 8 3 3 8 8 2 2 3 8 8 3 8 8 8 8 2 2 2 8 8 2 8 3 2 3\n",
      " 8 2 8 3 3 8 3 2 8 8 2 8 8 8 3 8 3]\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "step 5-9: studnt mean loss = 11.735017\n",
      "step 6: generator mean loss = -11.144072\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-69fcd9a275ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[1;31m# The grad for student\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstudent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m             \u001b[1;31m#grads, _ = tf.clip_by_global_norm(grads, 5.0)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[1;31m# Apply grad for student\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1012\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[0;32m   1015\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     74\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\ops\\nn_grad.py\u001b[0m in \u001b[0;36m_Conv2DGrad\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m    594\u001b[0m           \u001b[0mexplicit_paddings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexplicit_paddings\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m           \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 596\u001b[1;33m           data_format=data_format),\n\u001b[0m\u001b[0;32m    597\u001b[0m       gen_nn_ops.conv2d_backprop_filter(\n\u001b[0;32m    598\u001b[0m           \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d_backprop_input\u001b[1;34m(input_sizes, filter, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[0;32m   1353\u001b[0m         \u001b[1;34m\"use_cudnn_on_gpu\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"padding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1354\u001b[0m         \u001b[1;34m\"explicit_paddings\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexplicit_paddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"data_format\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1355\u001b[1;33m         \"dilations\", dilations)\n\u001b[0m\u001b[0;32m   1356\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1357\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# collect noise samples here\n",
    "allZ = []\n",
    "r=[]\n",
    "for total_batches in range(total_n_pseudo_batches):\n",
    "    # sample from latern space to make an image\n",
    "    z = tf.random.normal([batch_size, z_dim])\n",
    "    \n",
    "    allZ.append(z)\n",
    "    #pseudo_imgs = generator(z)\n",
    "    # Generator training\n",
    "    generator.trainable = True\n",
    "    student.trainable = False\n",
    "    with tf.GradientTape() as tape:\n",
    "        pseudo_imgs = generator(z)\n",
    "        t_logits, *t_acts = teacher(pseudo_imgs)\n",
    "        s_logits, *_ = student(pseudo_imgs)\n",
    "\n",
    "        # calculate the generator loss\n",
    "        sloss = generator_loss_fn(t_logits, s_logits)\n",
    "\n",
    "        # The grad for generator\n",
    "        grads = tape.gradient(sloss, generator.trainable_weights)\n",
    "        #grads, _ = tf.clip_by_global_norm(grads, 5.0)\n",
    "        #generator_optimizer.minimize(sloss_fn, var_list_fn)\n",
    "        #generator_optimizer.get_gradients(loss, generator.trainable_weights)\n",
    "        # update the generator paramter with the gradient\n",
    "        for ng in range(ng_batches):\n",
    "            generator_optimizer.apply_gradients(zip(grads, generator.trainable_weights))\n",
    "\n",
    "        g_loss_met(sloss)\n",
    "\n",
    "    print('step %s: generator mean loss = %s' % (total_batches, g_loss_met.result().numpy()))\n",
    "    # ==========================================================================\n",
    "\n",
    "    # Student training\n",
    "    generator.trainable = False\n",
    "    student.trainable = True\n",
    "    for ns in range(ns_batches):\n",
    "        #print(pseudo_imgs.shape)\n",
    "\n",
    "        #t_logits, *t_acts = teacher(pseudo_imgs)\n",
    "        with tf.GradientTape() as tape:\n",
    "            pseudo_imgs = generator(z)\n",
    "            t_logits, *t_acts = teacher(pseudo_imgs)\n",
    "            s_logits, *s_acts = student(pseudo_imgs)\n",
    "            loss = student_loss_fn(t_logits, t_acts, s_logits, s_acts, attn_beta)\n",
    "\n",
    "            # The grad for student\n",
    "            grads = tape.gradient(loss, student.trainable_weights)\n",
    "            #grads, _ = tf.clip_by_global_norm(grads, 5.0)\n",
    "            # Apply grad for student\n",
    "            student_optimizer.apply_gradients(zip(grads, student.trainable_weights))\n",
    "\n",
    "            stu_loss_met(loss)\n",
    "    print(np.argmax(t_logits, axis=-1))\n",
    "    print(np.argmax(s_logits, axis=-1))\n",
    "    print('step %s-%s: studnt mean loss = %s' % (total_batches, ns, stu_loss_met.result().numpy()))\n",
    "    r.append([g_loss_met.result().numpy(),stu_loss_met.result().numpy()])\n",
    "    stu_loss_met.reset_states()\n",
    "    g_loss_met.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-2.282476, 1.0006514],\n",
       " [-0.030416649, 0.30817515],\n",
       " [-0.17294392, 9.632488],\n",
       " [-9.238077, 3.0614452],\n",
       " [-3.2060957, 3.8781276],\n",
       " [-2.8374429, 11.735017]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x23ed06d90f0>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as q\n",
    "q.plot(np.array(r)[:25,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x23ed0e0c978>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "q.plot(np.array(r)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect noise samples here\n",
    "allZ = []\n",
    "\n",
    "for total_batches in range(total_n_pseudo_batches):\n",
    "    # sample from latern space to make an image\n",
    "    z = tf.random.normal([batch_size, z_dim])\n",
    "    allZ.append(z)\n",
    "    #pseudo_imgs = generator(z)\n",
    "    # Generator training\n",
    "    generator.trainable = True\n",
    "    student.trainable = False\n",
    "    for ng in range(ng_batches):\n",
    "        with tf.GradientTape() as tape:\n",
    "            pseudo_imgs = generator(z)\n",
    "            t_logits, *t_acts = teacher(pseudo_imgs)\n",
    "            s_logits, *_ = student(pseudo_imgs)\n",
    "\n",
    "            # calculate the generator loss\n",
    "            sloss = generator_loss_fn(t_logits, s_logits)\n",
    "\n",
    "            # The grad for generator\n",
    "            grads = tape.gradient(sloss, generator.trainable_weights)\n",
    "            grads, _ = tf.clip_by_global_norm(grads, 5.0)\n",
    "            #generator_optimizer.minimize(sloss_fn, var_list_fn)\n",
    "            #generator_optimizer.get_gradients(loss, generator.trainable_weights)\n",
    "            # update the generator paramter with the gradient\n",
    "            generator_optimizer.apply_gradients(zip(grads, generator.trainable_weights))\n",
    "\n",
    "            g_loss_met(sloss)\n",
    "\n",
    "    print('step %s: generator mean loss = %s' % (total_batches, g_loss_met.result().numpy()))\n",
    "    # ==========================================================================\n",
    "\n",
    "    # Student training\n",
    "    generator.trainable = False\n",
    "    student.trainable = True\n",
    "    for ns in range(ns_batches):\n",
    "\n",
    "        #t_logits, *t_acts = teacher(pseudo_imgs)\n",
    "        with tf.GradientTape() as tape:\n",
    "            s_logits, *s_acts = student(pseudo_imgs)\n",
    "            loss = student_loss_fn(t_logits, t_acts, s_logits, s_acts, attn_beta)\n",
    "\n",
    "        # The grad for student\n",
    "        grads = tape.gradient(loss, student.trainable_weights)\n",
    "        grads, _ = tf.clip_by_global_norm(grads, 5.0)\n",
    "        # Apply grad for student\n",
    "        student_optimizer.apply_gradients(zip(grads, student.trainable_weights))\n",
    "\n",
    "        stu_loss_met(loss)\n",
    "    print(np.argmax(t_logits, axis=-1))\n",
    "    print(np.argmax(s_logits, axis=-1))\n",
    "    print('step %s-%s: studnt mean loss = %s' % (total_batches, ns, stu_loss_met.result().numpy()))\n",
    "    stu_loss_met.reset_states()\n",
    "    g_loss_met.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=315071, shape=(128, 100), dtype=float32, numpy=\n",
       "array([[-1.0358686 , -0.684871  , -0.16783904, ...,  0.5143277 ,\n",
       "         0.2967401 , -0.5969298 ],\n",
       "       [-0.49305704,  0.93418497, -0.46287966, ..., -0.67069745,\n",
       "        -2.2316628 , -0.8568561 ],\n",
       "       [ 0.04292057, -2.0419998 ,  0.15681241, ...,  2.5317209 ,\n",
       "        -1.2869126 , -0.20450641],\n",
       "       ...,\n",
       "       [-1.1478996 ,  0.12412108, -0.7699375 , ..., -0.56207997,\n",
       "        -0.7943802 ,  0.660088  ],\n",
       "       [-0.19442326,  0.25211254,  0.87517554, ..., -0.7135385 ,\n",
       "         1.5305002 , -1.479154  ],\n",
       "       [-1.0420706 ,  1.6506463 ,  0.9232475 , ..., -0.73600006,\n",
       "        -2.8491    , -0.08360916]], dtype=float32)>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allZ[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=327085, shape=(128, 100), dtype=float32, numpy=\n",
       "array([[-9.7563308e-01,  4.7894308e-01,  8.5420752e-01, ...,\n",
       "        -1.5680150e+00,  1.3871658e+00,  2.9061723e-01],\n",
       "       [ 7.2446656e-01,  7.8352824e-02, -1.5123192e+00, ...,\n",
       "        -5.1941746e-01,  1.7411921e-03, -2.4970953e+00],\n",
       "       [-1.0857877e+00,  3.2333916e-01, -1.6581066e-01, ...,\n",
       "         6.6474557e-01,  1.3182930e+00,  1.6273452e+00],\n",
       "       ...,\n",
       "       [ 5.2141696e-01,  4.2684895e-01, -5.9079808e-01, ...,\n",
       "         1.1710562e+00, -9.2736143e-01,  4.9402893e-01],\n",
       "       [-1.2198722e+00,  6.7198426e-01,  1.1385178e-02, ...,\n",
       "         7.1641564e-01,  6.5333354e-01,  4.0541831e-01],\n",
       "       [ 5.6603950e-01,  2.7112632e+00,  4.9270988e-01, ...,\n",
       "         1.2977178e+00,  7.7176112e-01,  6.8481848e-02]], dtype=float32)>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allZ[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = tf.random.normal([128, z_dim])\n",
    "pseudo_imgs = generator(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_logits, *t_acts = teacher(pseudo_imgs)\n",
    "#s_logits, *s_acts = student(pseudo_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(t_logits, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 8 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-b0198c601842>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz_dim\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mpseudo_imgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mt_logits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mt_acts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mteacher\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpseudo_imgs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt_logits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[0;32m    890\u001b[0m               self._compute_dtype):\n\u001b[1;32m--> 891\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    z = tf.random.normal([128, z_dim])\n",
    "    pseudo_imgs = generator(z)\n",
    "    t_logits, *t_acts = teacher(pseudo_imgs)\n",
    "    print(np.argmax(t_logits, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3], dtype=int64)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(s_logits, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=622519, shape=(128,), dtype=float32, numpy=\n",
       "array([-16.108494, -16.109707, -16.10695 , -16.114367, -16.10838 ,\n",
       "       -16.112377, -16.107656, -16.105385, -16.110136, -16.104376,\n",
       "       -16.112566, -16.108063, -16.113611, -16.104847, -16.114328,\n",
       "       -16.111156, -16.109795, -16.104095, -16.112213, -16.113987,\n",
       "       -16.096975, -16.110203, -16.110744, -16.111609, -16.110949,\n",
       "       -16.105827, -16.11392 , -16.113228, -16.109297, -16.113071,\n",
       "       -16.11231 , -16.110935, -16.109127, -16.109371, -16.109913,\n",
       "       -16.112408, -16.107811, -16.10948 , -16.113201, -16.109617,\n",
       "       -16.106361, -16.108526, -16.108309, -16.109287, -16.111029,\n",
       "       -16.111767, -16.11352 , -16.112497, -16.112753, -16.10953 ,\n",
       "       -16.111723, -16.110271, -16.11001 , -16.10771 , -16.114899,\n",
       "       -16.100468, -16.104774, -16.112808, -16.106663, -16.10405 ,\n",
       "       -16.112036, -16.11499 , -16.108482, -16.112381, -16.112026,\n",
       "       -16.108225, -16.112162, -16.110857, -16.112865, -16.105967,\n",
       "       -16.11247 , -16.110826, -16.113087, -16.114857, -16.1112  ,\n",
       "       -16.10677 , -16.107578, -16.110636, -16.115143, -16.113884,\n",
       "       -16.113125, -16.113895, -16.113611, -16.110146, -16.111431,\n",
       "       -16.113049, -16.11357 , -16.11274 , -16.114454, -16.106365,\n",
       "       -16.114708, -16.109743, -16.102415, -16.098055, -16.111042,\n",
       "       -16.10801 , -16.105753, -16.109257, -16.109802, -16.111572,\n",
       "       -16.109512, -16.109053, -16.11003 , -16.102068, -16.107237,\n",
       "       -16.108328, -16.107992, -16.10813 , -16.108599, -16.104424,\n",
       "       -16.1112  , -16.112022, -16.110132, -16.112621, -16.1098  ,\n",
       "       -16.11003 , -16.110657, -16.112518, -16.107107, -16.113623,\n",
       "       -16.111069, -16.109783, -16.109762, -16.11112 , -16.112223,\n",
       "       -16.10762 , -16.113657, -16.10719 ], dtype=float32)>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-tf.keras.losses.kld(tf.math.softmax(t_logits/1), tf.math.softmax(s_logits/1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=619262, shape=(128, 10), dtype=float32, numpy=\n",
       "array([[1.73236019e-13, 5.79287418e-09, 9.99720991e-01, ...,\n",
       "        4.44518608e-13, 1.70145104e-05, 2.86930032e-07],\n",
       "       [1.80258721e-13, 5.59302693e-09, 9.99660611e-01, ...,\n",
       "        6.18259675e-13, 1.23469435e-05, 4.27991495e-07],\n",
       "       [1.41766265e-13, 5.77035930e-09, 9.99742448e-01, ...,\n",
       "        4.22092726e-13, 1.65913025e-05, 2.08873786e-07],\n",
       "       ...,\n",
       "       [2.67805445e-13, 6.51081722e-09, 9.99684572e-01, ...,\n",
       "        8.87235055e-13, 2.01805778e-05, 2.69925721e-07],\n",
       "       [2.05917816e-13, 5.73566616e-09, 9.99714553e-01, ...,\n",
       "        7.36191761e-13, 2.07085886e-05, 1.60248504e-07],\n",
       "       [1.41627270e-13, 6.21082918e-09, 9.99682784e-01, ...,\n",
       "        4.30357600e-13, 1.29865775e-05, 4.32773618e-07]], dtype=float32)>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.softmax(t_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([128, 10])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 8, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3], dtype=int64)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(t_logits, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3], dtype=int64)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(s_logits, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=618202, shape=(128,), dtype=float32, numpy=\n",
       "array([-16.113594, -16.113438, -16.105751, -16.11192 , -16.103193,\n",
       "       -16.112785, -16.109215, -16.10962 , -16.110657, -16.113207,\n",
       "       -16.106478, -16.107164, -16.109896, -16.112822, -16.110554,\n",
       "       -16.109566, -16.11013 , -16.111303, -16.10878 , -16.110264,\n",
       "       -16.113176, -16.113718, -16.112478, -16.1068  , -16.111942,\n",
       "       -16.107141, -16.105663, -16.108484, -16.109747, -16.109455,\n",
       "       -16.11235 , -16.114386, -16.108665, -16.110588, -16.11215 ,\n",
       "       -16.108305, -16.109087, -16.10959 , -16.10536 , -16.110159,\n",
       "       -16.114157, -16.105764, -16.110926, -16.105675, -16.107052,\n",
       "       -16.108885, -16.104893, -16.110935, -16.113972, -16.112383,\n",
       "       -16.113396, -16.112322, -16.10848 , -16.108068, -16.108719,\n",
       "       -16.111391, -16.106165, -16.100979, -16.11092 , -16.112635,\n",
       "       -16.114119, -16.1098  , -16.111538, -16.110523, -16.112242,\n",
       "       -16.113146, -16.111383, -16.107435, -16.10939 , -16.113394,\n",
       "       -16.109928, -16.111853, -16.11035 , -16.109745, -16.111876,\n",
       "       -16.1132  , -16.11194 , -16.114477, -16.108644, -16.111242,\n",
       "       -16.107868, -16.098757, -16.10026 , -16.111736, -16.108927,\n",
       "       -16.110563, -16.100569, -16.109123, -16.112764, -16.112444,\n",
       "       -16.111217, -16.110962, -16.112186, -16.109539, -16.113098,\n",
       "       -16.111904, -16.109694, -16.110168, -16.108713, -16.11505 ,\n",
       "       -16.110167, -16.112719, -16.11381 , -16.113428, -16.113432,\n",
       "       -16.110313, -16.109476, -16.110323, -16.111988, -16.110085,\n",
       "       -16.114395, -16.112028, -16.114576, -16.109943, -16.109095,\n",
       "       -16.099527, -16.111658, -16.110504, -16.111065, -16.112202,\n",
       "       -16.109934, -16.102505, -16.112867, -16.105534, -16.10268 ,\n",
       "       -16.111221, -16.099503, -16.11362 ], dtype=float32)>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-tf.keras.losses.kld(tf.math.softmax(t_logits/1), tf.math.softmax(s_logits/1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=8035, shape=(10,), dtype=float32, numpy=\n",
       "array([6.0265061e-06, 1.8073046e-05, 9.2791719e-04, 9.9107856e-01,\n",
       "       5.3548539e-04, 7.1743317e-04, 9.7325142e-04, 4.1424406e-07,\n",
       "       5.7407082e-03, 2.1374408e-06], dtype=float32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.softmax(t_logits, axis=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=299831, shape=(10,), dtype=float32, numpy=\n",
       "array([-4.3618917 , -8.372073  ,  0.17348364, 14.385825  , -6.7830644 ,\n",
       "       -8.034959  , -5.2229996 , -7.9174027 ,  0.16598235, -2.6487436 ],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_logits[66]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=299823, shape=(10,), dtype=float32, numpy=\n",
       "array([3.99472205e-10, 2.03878251e-11, 1.06562815e-07, 9.99999881e-01,\n",
       "       1.60894898e-13, 1.26279920e-09, 9.63189354e-14, 3.97031156e-16,\n",
       "       1.53560169e-08, 1.24514396e-12], dtype=float32)>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.softmax(t_logits)[66]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=299818, shape=(10,), dtype=float32, numpy=\n",
       "array([7.2105664e-09, 1.3072833e-10, 6.7244684e-07, 9.9999857e-01,\n",
       "       6.4042349e-10, 1.8313721e-10, 3.0478569e-09, 2.0598252e-10,\n",
       "       6.6742132e-07, 3.9992649e-08], dtype=float32)>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.softmax(s_logits)[66]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.7291103e-06"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(-tf.keras.losses.kld(tf.math.softmax(t_logits/1), tf.math.softmax(s_logits/1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=299768, shape=(128,), dtype=float32, numpy=\n",
       "array([-7.1930992e-07, -1.4643133e-06, -3.9379993e-06, -1.1229838e-06,\n",
       "       -1.3483844e-06, -2.1163869e-06, -1.7904100e-06, -1.2343418e-06,\n",
       "       -8.3294566e-07, -2.9433027e-06, -2.3060079e-06, -2.3060886e-06,\n",
       "       -4.6132254e-06, -4.0136135e-07, -2.1978351e-06, -1.1228403e-06,\n",
       "       -1.9052153e-06, -9.9882197e-07, -1.1318238e-06, -2.9723235e-06,\n",
       "       -2.9776870e-06, -4.5917216e-07, -6.3910602e-07, -1.1353343e-06,\n",
       "       -1.5817259e-06, -2.1214680e-06, -1.1263053e-06, -2.1903036e-06,\n",
       "       -3.0410292e-06, -1.5713857e-06, -4.9523514e-06, -5.0400854e-07,\n",
       "       -2.2257009e-06, -1.1359920e-06, -1.4554205e-06, -9.0088275e-07,\n",
       "       -2.1262892e-06, -1.0383005e-06, -2.0847315e-06, -1.6683788e-06,\n",
       "       -1.1373497e-06, -1.0234830e-06, -4.1982997e-07, -5.9526428e-07,\n",
       "       -9.1840212e-07, -8.2094726e-07, -1.4476336e-06, -8.9536110e-07,\n",
       "       -3.5031110e-06, -6.2645017e-07, -1.1316400e-06, -2.8427962e-06,\n",
       "       -2.4084102e-06, -8.9444444e-07, -2.2089948e-06, -1.6843578e-06,\n",
       "       -3.7260088e-06, -7.1907846e-07, -8.1945603e-07, -3.7112425e-06,\n",
       "       -3.3992667e-06, -2.2530342e-06, -7.2291670e-07, -2.0931823e-06,\n",
       "       -6.3432520e-07, -8.1064388e-07, -9.2516729e-07, -4.0500313e-06,\n",
       "       -1.1158136e-06, -3.1828856e-06, -4.7168355e-06, -1.1232976e-06,\n",
       "       -8.2373697e-07, -1.1172542e-06, -3.3435853e-07, -5.2419250e-07,\n",
       "       -1.5625054e-06, -3.0712749e-06, -7.1791510e-07, -3.7280633e-06,\n",
       "       -4.1328468e-07, -1.2365682e-06, -2.8540853e-06, -2.1240890e-06,\n",
       "       -3.4901445e-06, -4.4526490e-07, -8.3331832e-07, -2.1198607e-06,\n",
       "       -2.8505860e-06, -1.1278457e-06, -1.3531985e-06, -1.5734909e-06,\n",
       "       -6.8735631e-07, -1.5708139e-06, -3.1635120e-06, -4.7302169e-06,\n",
       "       -1.7831838e-06, -1.7818662e-06, -5.6323199e-07, -1.7781831e-06,\n",
       "       -2.1245974e-06, -1.0769035e-06, -2.7505916e-06, -2.0006182e-06,\n",
       "       -2.4407352e-06, -2.9644480e-06, -9.1457548e-07, -9.0887568e-07,\n",
       "       -8.3805912e-07, -9.1582757e-07, -7.8425029e-07, -1.6791467e-06,\n",
       "       -1.7976540e-06, -2.5393804e-06, -1.1187427e-06, -1.1557973e-06,\n",
       "       -9.1175315e-07, -1.1234498e-06, -5.3050479e-07, -8.2174626e-07,\n",
       "       -5.4046677e-07, -1.2845269e-06, -8.1080361e-07, -5.0590279e-06,\n",
       "       -5.6362529e-07, -2.2225074e-06, -2.2202155e-06, -1.9047359e-06],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-tf.keras.losses.kld(tf.math.softmax(t_logits/1), tf.math.softmax(s_logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=8284, shape=(10,), dtype=float32, numpy=\n",
       "array([0.09966495, 0.10159311, 0.1002707 , 0.09997319, 0.09979188,\n",
       "       0.1008326 , 0.09929379, 0.09801106, 0.10015489, 0.10041383],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.softmax(s_logits)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.preprocess import load_cifar10_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = load_cifar10_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(student):\n",
    "    model = WideResidualNetwork(16, 1, input_shape=(32, 32, 3), dropout_rate=0.0)\n",
    "    model.set_weights(student.get_weights()) \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "    loss, accuracy = model.evaluate(x_test, y_test, batch_size=128, verbose=0)\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "call() got an unexpected keyword argument 'batch_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-962a266ee134>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mstest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstudent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[0;32m    890\u001b[0m               self._compute_dtype):\n\u001b[1;32m--> 891\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: call() got an unexpected keyword argument 'batch_size'"
     ]
    }
   ],
   "source": [
    "stest = student(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.73 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(29.924496099853517, 0.1)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "get_accuracy(student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "student.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = True\n",
    "b = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "if a and not b:\n",
    "    print(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 577 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = WideResidualNetwork(16, 1, input_shape=(32, 32, 3), dropout_rate=0.0)\n",
    "model.set_weights(student.get_weights()) \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 2.3115152214050294\n",
      "Test accuracy: 0.1098\n",
      "Wall time: 7.44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0%11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10%11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "11%11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "12%11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1%11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen update\n",
      "student update\n",
      "student update\n",
      "student update\n",
      "student update\n",
      "student update\n",
      "student update\n",
      "student update\n",
      "student update\n",
      "student update\n",
      "student update\n",
      "gen update\n",
      "student update\n",
      "student update\n",
      "student update\n",
      "student update\n",
      "student update\n",
      "student update\n",
      "student update\n",
      "student update\n",
      "student update\n",
      "student update\n"
     ]
    }
   ],
   "source": [
    "ip = 0\n",
    "npb = 0\n",
    "tnpb = 2\n",
    "nrb = 11\n",
    "ng = 1\n",
    "ns = 10\n",
    "while npb < tnpb:\n",
    "    if ip % nrb < ng:\n",
    "        print(\"gen update\")\n",
    "    elif ip % nrb < (ng+ns):\n",
    "        print(\"student update\")\n",
    "    if (ip+1) % nrb == 0:\n",
    "        npb += 1\n",
    "    ip += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80000.0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8e4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_optimizer.learning_rate(80001).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=8540, shape=(128, 32, 32, 3), dtype=float32, numpy=\n",
       "array([[[[ 4.22538724e-03,  5.52038196e-03,  2.78851506e-03],\n",
       "         [ 1.51259303e-02, -7.12775532e-03, -8.53509177e-03],\n",
       "         [ 2.65080691e-03, -1.44896386e-02, -4.11295658e-03],\n",
       "         ...,\n",
       "         [-3.22682608e-04, -9.70256235e-03,  3.45329754e-03],\n",
       "         [-8.36795475e-03, -1.96127128e-02,  7.81526230e-03],\n",
       "         [-1.02375029e-03, -1.18294591e-02,  1.71103310e-02]],\n",
       "\n",
       "        [[-2.88313581e-03,  4.60245833e-03, -4.73300321e-03],\n",
       "         [-2.09012460e-02,  8.04036763e-03,  4.24172403e-03],\n",
       "         [-1.87908225e-02, -2.52782665e-02, -1.08433794e-03],\n",
       "         ...,\n",
       "         [-1.77930836e-02,  1.47952419e-02, -1.38257826e-02],\n",
       "         [-1.23509280e-02, -2.97810733e-02, -2.96389940e-03],\n",
       "         [-8.16030428e-03, -4.22601867e-03, -4.11631120e-03]],\n",
       "\n",
       "        [[ 1.00406213e-02, -1.96873280e-03, -2.75179697e-03],\n",
       "         [ 2.50531198e-03, -1.12852873e-02,  1.28602535e-02],\n",
       "         [ 1.53854266e-02, -1.00648580e-02,  2.58663427e-02],\n",
       "         ...,\n",
       "         [-9.23773088e-03,  8.78068805e-03, -2.14134492e-02],\n",
       "         [-5.92796598e-03, -1.51873054e-02, -9.53547365e-04],\n",
       "         [ 2.87550385e-03, -2.04208237e-03, -2.53307773e-03]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 3.50258052e-02, -9.47078224e-03,  2.04642750e-02],\n",
       "         [ 8.93052295e-03,  1.94647089e-02, -1.32740401e-02],\n",
       "         [-2.14493237e-02,  3.70727037e-03, -2.12025177e-02],\n",
       "         ...,\n",
       "         [ 4.21136385e-03, -3.98423746e-02, -5.22374595e-03],\n",
       "         [ 3.33301239e-02, -3.70700210e-02,  2.67462339e-02],\n",
       "         [ 1.09170927e-02, -5.74440230e-03, -1.07264053e-02]],\n",
       "\n",
       "        [[-4.91142552e-03, -5.16236527e-03, -1.40711833e-02],\n",
       "         [-3.15742642e-02, -1.26714734e-02,  1.53959813e-02],\n",
       "         [-1.12420591e-02, -5.20999404e-03, -1.47951758e-02],\n",
       "         ...,\n",
       "         [ 1.82032678e-02, -8.57503712e-03,  1.88566111e-02],\n",
       "         [-2.04696190e-02, -1.70422159e-02, -2.39612348e-03],\n",
       "         [-3.10745649e-02, -2.10616495e-02,  1.65646803e-02]],\n",
       "\n",
       "        [[-7.66820973e-03, -4.55719233e-03, -3.41306045e-03],\n",
       "         [-3.46073089e-03,  2.99223289e-02,  8.13504215e-03],\n",
       "         [ 1.48410527e-02, -1.00240586e-02, -2.67144926e-02],\n",
       "         ...,\n",
       "         [ 1.06941387e-02, -1.27903549e-02,  1.00168800e-02],\n",
       "         [ 3.35857528e-03, -1.01660667e-02,  5.19121881e-04],\n",
       "         [-1.11579858e-02, -1.39524257e-02, -1.83610804e-02]]],\n",
       "\n",
       "\n",
       "       [[[ 1.26030017e-02, -5.24941366e-03,  2.64735427e-03],\n",
       "         [ 5.11650182e-03,  5.56886150e-03,  1.89172442e-03],\n",
       "         [ 1.46040237e-02,  1.17149181e-03,  1.81123894e-02],\n",
       "         ...,\n",
       "         [ 1.17509328e-02,  6.05449500e-03,  2.57472787e-02],\n",
       "         [ 1.69781856e-02, -2.83142533e-02,  2.83331368e-02],\n",
       "         [ 6.46868115e-03, -4.65068640e-03,  1.83398426e-02]],\n",
       "\n",
       "        [[ 1.13789653e-02, -1.02632092e-02, -4.40216361e-04],\n",
       "         [-2.39376724e-02, -6.74951868e-03,  7.72996806e-03],\n",
       "         [-1.93196367e-02, -1.92293711e-02,  4.98754065e-03],\n",
       "         ...,\n",
       "         [ 1.45256780e-02,  2.58140871e-03,  1.56775061e-02],\n",
       "         [-3.01100872e-02, -1.87134463e-02,  1.55193107e-02],\n",
       "         [-6.54316973e-03, -1.54666444e-02,  7.69593474e-03]],\n",
       "\n",
       "        [[-1.43025555e-02, -5.44389430e-03,  1.40125782e-03],\n",
       "         [-2.30917963e-03, -1.03688156e-02,  3.23212123e-03],\n",
       "         [-7.42717693e-03, -2.13558264e-02,  1.12704039e-02],\n",
       "         ...,\n",
       "         [ 1.07316012e-02, -1.00278901e-02, -1.17809302e-03],\n",
       "         [-3.24324612e-03, -4.15733419e-02, -1.43980584e-03],\n",
       "         [-1.87320132e-02, -1.06049795e-02, -3.29275173e-03]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1.90251842e-02, -1.56078702e-02, -2.11444078e-03],\n",
       "         [ 9.73566715e-03,  1.76778657e-03, -9.66315437e-03],\n",
       "         [ 1.54349615e-03,  4.36925802e-05, -1.57184321e-02],\n",
       "         ...,\n",
       "         [ 2.31958982e-02,  5.12939394e-02, -6.19489364e-02],\n",
       "         [-1.98570662e-03, -5.18794842e-02,  9.40036122e-03],\n",
       "         [ 1.34533634e-02, -1.76180471e-02,  3.13484930e-02]],\n",
       "\n",
       "        [[-7.25160958e-03,  4.91497852e-03,  5.19350590e-03],\n",
       "         [-1.12687482e-03, -1.99920824e-03,  1.46439653e-02],\n",
       "         [ 1.12052532e-02, -2.98118256e-02, -1.37200542e-02],\n",
       "         ...,\n",
       "         [ 1.92116760e-02, -3.43176015e-02,  1.76577624e-02],\n",
       "         [-6.72981003e-03, -1.32242637e-02,  9.41653922e-03],\n",
       "         [-1.44866100e-02, -1.04448441e-02,  4.65012901e-02]],\n",
       "\n",
       "        [[ 1.31530128e-03, -1.84054126e-03,  2.67901155e-03],\n",
       "         [ 1.88816674e-02, -8.46746285e-03,  1.79421958e-02],\n",
       "         [-3.82743850e-02, -1.85890179e-02, -1.61122885e-02],\n",
       "         ...,\n",
       "         [ 2.93157529e-02, -7.73712434e-03, -5.71893109e-03],\n",
       "         [ 1.73165668e-02, -1.70549992e-02, -2.09183190e-02],\n",
       "         [-8.77807289e-03,  2.87755718e-03,  1.00418483e-03]]],\n",
       "\n",
       "\n",
       "       [[[ 1.18786695e-04, -2.86068837e-03,  2.38358253e-03],\n",
       "         [ 1.64404139e-02, -2.72520003e-03,  6.35374524e-03],\n",
       "         [ 4.65627667e-03, -1.04197487e-02,  1.27394097e-02],\n",
       "         ...,\n",
       "         [ 2.31320076e-02,  2.87273177e-03,  1.69234793e-03],\n",
       "         [ 1.05445134e-03, -8.85738339e-03,  2.05029286e-02],\n",
       "         [-5.13769174e-03,  4.72501386e-03,  5.68725495e-03]],\n",
       "\n",
       "        [[ 2.45783129e-03,  5.39597997e-04, -3.47720715e-03],\n",
       "         [-5.37440239e-04, -1.20318856e-03,  5.46490715e-04],\n",
       "         [-1.26621956e-02, -9.87099702e-05,  1.86716020e-02],\n",
       "         ...,\n",
       "         [ 3.57456831e-03,  1.87431239e-02, -2.35487446e-02],\n",
       "         [ 4.19129431e-02, -5.73170930e-03,  1.11030564e-02],\n",
       "         [-2.82595940e-02, -3.05940229e-02,  5.03057148e-03]],\n",
       "\n",
       "        [[ 3.96103971e-03,  1.18934794e-03, -4.38705925e-03],\n",
       "         [ 2.33696750e-03, -1.38584282e-02, -1.73830788e-03],\n",
       "         [-3.57772619e-03, -9.71495453e-03, -1.24522438e-03],\n",
       "         ...,\n",
       "         [-4.39267745e-03,  6.52109273e-04,  1.87455993e-02],\n",
       "         [-1.04368972e-02, -6.63320348e-03, -1.97505895e-02],\n",
       "         [ 1.13159223e-02, -1.52703654e-02,  3.28047900e-03]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 1.52147969e-03, -4.14861785e-03, -5.75223099e-03],\n",
       "         [-8.07297882e-03, -7.19792303e-03,  1.74659304e-02],\n",
       "         [-1.72719341e-02,  1.06310928e-02, -2.12827064e-02],\n",
       "         ...,\n",
       "         [-4.33466062e-02, -8.36445106e-05, -3.14062042e-03],\n",
       "         [ 2.23260056e-02,  8.79895315e-03,  2.19785683e-02],\n",
       "         [-1.55560337e-02,  4.65839542e-03,  3.66170257e-02]],\n",
       "\n",
       "        [[-5.72489435e-03,  2.55657290e-03, -2.41620070e-03],\n",
       "         [ 3.79269291e-03, -7.38277985e-03,  9.56770126e-03],\n",
       "         [ 2.84376666e-02, -1.78454053e-02, -1.69245396e-02],\n",
       "         ...,\n",
       "         [-2.60766316e-02,  1.85695745e-03,  6.54491186e-02],\n",
       "         [ 2.90232450e-02, -3.70598100e-02,  3.58518190e-03],\n",
       "         [-8.80284701e-03, -1.47646135e-02,  5.54344710e-03]],\n",
       "\n",
       "        [[ 2.46813917e-03,  7.09974905e-03,  1.21149933e-02],\n",
       "         [ 7.14335311e-03,  1.15430206e-02,  2.33440343e-02],\n",
       "         [-9.96346213e-03,  8.66522733e-03, -1.04661277e-02],\n",
       "         ...,\n",
       "         [ 1.62176564e-02, -5.08822389e-02,  6.86049927e-03],\n",
       "         [ 1.63856298e-02, -1.79925002e-02, -3.88504169e-03],\n",
       "         [ 1.35530403e-03, -2.16753897e-03, -1.26563255e-02]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[ 4.58033988e-03,  2.05036998e-03,  1.82649959e-03],\n",
       "         [ 2.25102832e-03,  8.82178359e-03,  1.46696216e-03],\n",
       "         [ 7.18876999e-03, -6.41817693e-03,  5.19541884e-03],\n",
       "         ...,\n",
       "         [ 7.32604461e-03,  2.02545151e-03,  7.62463408e-03],\n",
       "         [-2.34047566e-02, -1.34790037e-02,  2.85274368e-02],\n",
       "         [ 4.53784689e-03, -1.04379319e-02,  1.12456502e-03]],\n",
       "\n",
       "        [[ 4.53750556e-03,  6.11169729e-03, -1.06723630e-03],\n",
       "         [-1.11322738e-02, -1.07908330e-03,  2.71078106e-03],\n",
       "         [-6.21391693e-04, -1.75742730e-02, -1.11311907e-03],\n",
       "         ...,\n",
       "         [-7.76790176e-03,  1.94907673e-02,  9.35287401e-03],\n",
       "         [ 3.43191344e-03, -1.88489780e-02,  1.65805407e-02],\n",
       "         [-4.48594149e-03,  9.21433605e-03,  6.80170581e-03]],\n",
       "\n",
       "        [[-2.08454626e-03, -1.39505742e-03,  1.32841039e-02],\n",
       "         [ 5.10224607e-03, -1.76970586e-02,  9.95215308e-03],\n",
       "         [-7.79325655e-03, -3.16315070e-02,  7.29998015e-03],\n",
       "         ...,\n",
       "         [-8.23273044e-03, -1.32144224e-02,  3.48226316e-02],\n",
       "         [-4.54511959e-03, -1.83954891e-02,  2.46509425e-02],\n",
       "         [ 6.04538573e-03, -1.31596364e-02,  6.48135273e-03]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 2.22482253e-02, -2.96849385e-03, -1.16087105e-02],\n",
       "         [ 2.36431994e-02, -3.66061274e-03,  2.40264684e-02],\n",
       "         [-1.27387070e-03, -2.40727663e-02,  8.05993658e-03],\n",
       "         ...,\n",
       "         [ 2.17103269e-02, -1.72289356e-03,  2.13653897e-03],\n",
       "         [ 5.24418708e-03, -3.09215635e-02,  4.31867689e-02],\n",
       "         [ 7.94511102e-03,  3.58113274e-03,  1.26339484e-03]],\n",
       "\n",
       "        [[ 9.39511508e-03, -6.73198083e-04,  1.29924869e-04],\n",
       "         [ 1.23564424e-02, -9.93866939e-03,  1.78455506e-02],\n",
       "         [-1.81261934e-02,  2.07141950e-03,  2.00673193e-02],\n",
       "         ...,\n",
       "         [ 2.73169251e-03,  1.22112259e-02, -2.77400832e-03],\n",
       "         [-2.26768176e-03, -1.69754103e-02,  1.38694998e-02],\n",
       "         [ 5.88057563e-03,  2.87805987e-03,  2.48723128e-03]],\n",
       "\n",
       "        [[-9.10807867e-03, -1.27403555e-03,  1.50816166e-03],\n",
       "         [ 1.63247585e-02, -2.70837103e-03, -2.25967728e-02],\n",
       "         [ 8.86455271e-03, -3.51535268e-02, -1.33828428e-02],\n",
       "         ...,\n",
       "         [-4.36010910e-03,  9.62122460e-04, -5.28986566e-03],\n",
       "         [-2.36001071e-02, -1.11457007e-02, -2.20112526e-03],\n",
       "         [-2.53279638e-02,  4.48015472e-03, -7.99030531e-03]]],\n",
       "\n",
       "\n",
       "       [[[ 3.30007658e-03, -4.22224961e-03,  3.14445491e-03],\n",
       "         [ 9.68872337e-04,  7.46899983e-03,  3.52810603e-03],\n",
       "         [ 3.95566691e-03, -9.27371555e-04,  1.49654020e-02],\n",
       "         ...,\n",
       "         [-4.12277551e-03, -1.39618106e-02, -5.24718687e-03],\n",
       "         [ 3.00377561e-03, -2.62404066e-02,  1.66173663e-03],\n",
       "         [-2.76745413e-03, -1.43322423e-02,  2.58596502e-02]],\n",
       "\n",
       "        [[-1.37073710e-03, -5.21791447e-03, -2.98608816e-03],\n",
       "         [-6.55602291e-03, -2.24543037e-03, -1.69212185e-02],\n",
       "         [ 8.93704419e-04, -2.49905139e-02, -1.90514699e-02],\n",
       "         ...,\n",
       "         [ 9.71795060e-03, -8.55540577e-03, -3.89129645e-03],\n",
       "         [ 1.46011347e-02, -3.75774801e-02, -2.21156236e-03],\n",
       "         [ 3.69514618e-03, -1.10154203e-03,  1.40111730e-03]],\n",
       "\n",
       "        [[-1.45241839e-03, -3.64956283e-03, -1.10374447e-02],\n",
       "         [ 3.58364843e-02, -1.37780327e-02,  1.65765099e-02],\n",
       "         [ 2.21243780e-03, -1.33385351e-02, -1.04853027e-02],\n",
       "         ...,\n",
       "         [-1.60804596e-02,  1.71123464e-02, -1.67308580e-02],\n",
       "         [-1.66292046e-03, -2.14449409e-02, -8.12065601e-03],\n",
       "         [ 1.24695571e-02, -1.35801015e-02,  1.16327563e-02]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 1.13853365e-02,  7.94237480e-03, -3.63238482e-03],\n",
       "         [-4.08773161e-02,  3.54339927e-02,  1.03800287e-02],\n",
       "         [-1.60615593e-02, -7.87398871e-03, -3.47165130e-02],\n",
       "         ...,\n",
       "         [ 2.64455993e-02,  2.27998253e-02,  1.74080487e-02],\n",
       "         [ 4.89494652e-02, -1.08042676e-02,  1.97222922e-02],\n",
       "         [ 7.48272985e-03, -1.27018131e-02,  7.66233448e-03]],\n",
       "\n",
       "        [[ 6.07984047e-03,  9.75554343e-03, -3.71981366e-03],\n",
       "         [-6.29752502e-03, -1.65060889e-02,  1.94689017e-02],\n",
       "         [ 2.66990103e-02, -1.01191569e-02,  8.08395911e-03],\n",
       "         ...,\n",
       "         [-2.03365944e-02, -5.81614636e-02,  2.59566698e-02],\n",
       "         [-3.38796489e-02, -2.54054572e-02,  3.72719369e-03],\n",
       "         [-1.73988175e-02, -1.77465975e-02,  6.67454442e-03]],\n",
       "\n",
       "        [[ 1.07586579e-02,  2.25455873e-03,  8.06515757e-03],\n",
       "         [ 1.68004837e-02,  3.65148261e-02, -4.15276131e-03],\n",
       "         [-2.23210827e-03,  1.61663967e-03, -3.45313586e-02],\n",
       "         ...,\n",
       "         [-4.10880474e-03, -2.08449531e-02,  4.60341526e-03],\n",
       "         [ 1.59404725e-02, -1.43768694e-02, -9.14168172e-03],\n",
       "         [-5.80202509e-03,  1.09892886e-03, -2.88140355e-03]]],\n",
       "\n",
       "\n",
       "       [[[ 5.14219655e-03, -5.79495914e-03,  3.83384828e-03],\n",
       "         [ 1.48892589e-02,  6.69672678e-04,  1.22384848e-02],\n",
       "         [-2.78221304e-03, -1.21605927e-02,  7.59960711e-03],\n",
       "         ...,\n",
       "         [ 1.94661673e-02,  2.24036654e-03, -2.23111957e-02],\n",
       "         [-2.08906215e-02, -5.22269495e-03,  1.20225223e-02],\n",
       "         [ 9.79785807e-03,  6.34382898e-03,  1.81155559e-03]],\n",
       "\n",
       "        [[-1.34646741e-03,  7.31467083e-03,  8.47275369e-04],\n",
       "         [-8.35814327e-03,  1.17989704e-02,  2.51553743e-03],\n",
       "         [-9.35436971e-03, -1.40312621e-02,  2.48550922e-02],\n",
       "         ...,\n",
       "         [-1.10641401e-02,  4.72822366e-03, -1.49877407e-02],\n",
       "         [ 1.74596272e-02, -1.80577710e-02,  4.99122683e-03],\n",
       "         [-5.51839732e-03,  5.38448337e-03,  8.50686990e-03]],\n",
       "\n",
       "        [[ 6.46829931e-03,  4.86530690e-03,  8.69179517e-03],\n",
       "         [ 1.22568682e-02, -1.03525314e-02,  1.31883854e-02],\n",
       "         [-3.30386334e-04, -1.40224618e-03,  2.36864780e-05],\n",
       "         ...,\n",
       "         [-4.96978965e-03, -3.26548778e-02,  1.42678926e-02],\n",
       "         [ 1.92940272e-02, -3.28130871e-02,  6.02220744e-03],\n",
       "         [-1.33772828e-02, -9.97169968e-03,  1.12446183e-02]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-6.28691260e-03, -1.24011689e-03, -8.06987006e-03],\n",
       "         [-1.57387578e-03, -2.22597434e-03, -2.55087968e-02],\n",
       "         [ 1.45897036e-02, -2.35763807e-02,  5.62034547e-03],\n",
       "         ...,\n",
       "         [ 4.42876928e-02,  5.99564053e-02, -3.27207632e-02],\n",
       "         [-1.04976185e-02, -5.25745749e-02,  3.83359492e-02],\n",
       "         [-6.27105497e-03, -9.27061774e-03,  1.98149141e-02]],\n",
       "\n",
       "        [[-9.47710965e-03, -7.07879663e-03,  4.51566186e-04],\n",
       "         [ 1.02188429e-02, -2.52464935e-02,  3.46835777e-02],\n",
       "         [ 2.67429855e-02, -5.89789962e-03, -1.79437753e-02],\n",
       "         ...,\n",
       "         [-1.56744123e-02,  1.06822615e-02,  1.89642310e-02],\n",
       "         [-2.80948542e-03, -3.23147252e-02, -4.96364385e-03],\n",
       "         [ 7.05139106e-03, -4.20343950e-02,  2.77609322e-02]],\n",
       "\n",
       "        [[-6.41341694e-03, -6.85502542e-03, -1.28373718e-02],\n",
       "         [ 3.55625674e-02,  3.73105332e-03,  2.49457564e-02],\n",
       "         [-1.03214206e-02,  7.19720963e-03, -1.54823484e-02],\n",
       "         ...,\n",
       "         [ 1.43609829e-02, -1.00060469e-02, -1.67596843e-02],\n",
       "         [-3.90181760e-03, -2.67953780e-02, -5.23525663e-03],\n",
       "         [-4.59566992e-03,  8.86249449e-03, -3.57041904e-03]]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=8850, shape=(128, 32, 32, 3), dtype=float32, numpy=\n",
       "array([[[[ 4.22538724e-03,  5.52038196e-03,  2.78851506e-03],\n",
       "         [ 1.51259303e-02, -7.12775532e-03, -8.53509177e-03],\n",
       "         [ 2.65080691e-03, -1.44896386e-02, -4.11295658e-03],\n",
       "         ...,\n",
       "         [-3.22682608e-04, -9.70256235e-03,  3.45329754e-03],\n",
       "         [-8.36795475e-03, -1.96127128e-02,  7.81526230e-03],\n",
       "         [-1.02375029e-03, -1.18294591e-02,  1.71103310e-02]],\n",
       "\n",
       "        [[-2.88313581e-03,  4.60245833e-03, -4.73300321e-03],\n",
       "         [-2.09012460e-02,  8.04036763e-03,  4.24172403e-03],\n",
       "         [-1.87908225e-02, -2.52782665e-02, -1.08433794e-03],\n",
       "         ...,\n",
       "         [-1.77930836e-02,  1.47952419e-02, -1.38257826e-02],\n",
       "         [-1.23509280e-02, -2.97810733e-02, -2.96389940e-03],\n",
       "         [-8.16030428e-03, -4.22601867e-03, -4.11631120e-03]],\n",
       "\n",
       "        [[ 1.00406213e-02, -1.96873280e-03, -2.75179697e-03],\n",
       "         [ 2.50531198e-03, -1.12852873e-02,  1.28602535e-02],\n",
       "         [ 1.53854266e-02, -1.00648580e-02,  2.58663427e-02],\n",
       "         ...,\n",
       "         [-9.23773088e-03,  8.78068805e-03, -2.14134492e-02],\n",
       "         [-5.92796598e-03, -1.51873054e-02, -9.53547365e-04],\n",
       "         [ 2.87550385e-03, -2.04208237e-03, -2.53307773e-03]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 3.50258052e-02, -9.47078224e-03,  2.04642750e-02],\n",
       "         [ 8.93052295e-03,  1.94647089e-02, -1.32740401e-02],\n",
       "         [-2.14493237e-02,  3.70727037e-03, -2.12025177e-02],\n",
       "         ...,\n",
       "         [ 4.21136385e-03, -3.98423746e-02, -5.22374595e-03],\n",
       "         [ 3.33301239e-02, -3.70700210e-02,  2.67462339e-02],\n",
       "         [ 1.09170927e-02, -5.74440230e-03, -1.07264053e-02]],\n",
       "\n",
       "        [[-4.91142552e-03, -5.16236527e-03, -1.40711833e-02],\n",
       "         [-3.15742642e-02, -1.26714734e-02,  1.53959813e-02],\n",
       "         [-1.12420591e-02, -5.20999404e-03, -1.47951758e-02],\n",
       "         ...,\n",
       "         [ 1.82032678e-02, -8.57503712e-03,  1.88566111e-02],\n",
       "         [-2.04696190e-02, -1.70422159e-02, -2.39612348e-03],\n",
       "         [-3.10745649e-02, -2.10616495e-02,  1.65646803e-02]],\n",
       "\n",
       "        [[-7.66820973e-03, -4.55719233e-03, -3.41306045e-03],\n",
       "         [-3.46073089e-03,  2.99223289e-02,  8.13504215e-03],\n",
       "         [ 1.48410527e-02, -1.00240586e-02, -2.67144926e-02],\n",
       "         ...,\n",
       "         [ 1.06941387e-02, -1.27903549e-02,  1.00168800e-02],\n",
       "         [ 3.35857528e-03, -1.01660667e-02,  5.19121881e-04],\n",
       "         [-1.11579858e-02, -1.39524257e-02, -1.83610804e-02]]],\n",
       "\n",
       "\n",
       "       [[[ 1.26030017e-02, -5.24941366e-03,  2.64735427e-03],\n",
       "         [ 5.11650182e-03,  5.56886150e-03,  1.89172442e-03],\n",
       "         [ 1.46040237e-02,  1.17149181e-03,  1.81123894e-02],\n",
       "         ...,\n",
       "         [ 1.17509328e-02,  6.05449500e-03,  2.57472787e-02],\n",
       "         [ 1.69781856e-02, -2.83142533e-02,  2.83331368e-02],\n",
       "         [ 6.46868115e-03, -4.65068640e-03,  1.83398426e-02]],\n",
       "\n",
       "        [[ 1.13789653e-02, -1.02632092e-02, -4.40216361e-04],\n",
       "         [-2.39376724e-02, -6.74951868e-03,  7.72996806e-03],\n",
       "         [-1.93196367e-02, -1.92293711e-02,  4.98754065e-03],\n",
       "         ...,\n",
       "         [ 1.45256780e-02,  2.58140871e-03,  1.56775061e-02],\n",
       "         [-3.01100872e-02, -1.87134463e-02,  1.55193107e-02],\n",
       "         [-6.54316973e-03, -1.54666444e-02,  7.69593474e-03]],\n",
       "\n",
       "        [[-1.43025555e-02, -5.44389430e-03,  1.40125782e-03],\n",
       "         [-2.30917963e-03, -1.03688156e-02,  3.23212123e-03],\n",
       "         [-7.42717693e-03, -2.13558264e-02,  1.12704039e-02],\n",
       "         ...,\n",
       "         [ 1.07316012e-02, -1.00278901e-02, -1.17809302e-03],\n",
       "         [-3.24324612e-03, -4.15733419e-02, -1.43980584e-03],\n",
       "         [-1.87320132e-02, -1.06049795e-02, -3.29275173e-03]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1.90251842e-02, -1.56078702e-02, -2.11444078e-03],\n",
       "         [ 9.73566715e-03,  1.76778657e-03, -9.66315437e-03],\n",
       "         [ 1.54349615e-03,  4.36925802e-05, -1.57184321e-02],\n",
       "         ...,\n",
       "         [ 2.31958982e-02,  5.12939394e-02, -6.19489364e-02],\n",
       "         [-1.98570662e-03, -5.18794842e-02,  9.40036122e-03],\n",
       "         [ 1.34533634e-02, -1.76180471e-02,  3.13484930e-02]],\n",
       "\n",
       "        [[-7.25160958e-03,  4.91497852e-03,  5.19350590e-03],\n",
       "         [-1.12687482e-03, -1.99920824e-03,  1.46439653e-02],\n",
       "         [ 1.12052532e-02, -2.98118256e-02, -1.37200542e-02],\n",
       "         ...,\n",
       "         [ 1.92116760e-02, -3.43176015e-02,  1.76577624e-02],\n",
       "         [-6.72981003e-03, -1.32242637e-02,  9.41653922e-03],\n",
       "         [-1.44866100e-02, -1.04448441e-02,  4.65012901e-02]],\n",
       "\n",
       "        [[ 1.31530128e-03, -1.84054126e-03,  2.67901155e-03],\n",
       "         [ 1.88816674e-02, -8.46746285e-03,  1.79421958e-02],\n",
       "         [-3.82743850e-02, -1.85890179e-02, -1.61122885e-02],\n",
       "         ...,\n",
       "         [ 2.93157529e-02, -7.73712434e-03, -5.71893109e-03],\n",
       "         [ 1.73165668e-02, -1.70549992e-02, -2.09183190e-02],\n",
       "         [-8.77807289e-03,  2.87755718e-03,  1.00418483e-03]]],\n",
       "\n",
       "\n",
       "       [[[ 1.18786695e-04, -2.86068837e-03,  2.38358253e-03],\n",
       "         [ 1.64404139e-02, -2.72520003e-03,  6.35374524e-03],\n",
       "         [ 4.65627667e-03, -1.04197487e-02,  1.27394097e-02],\n",
       "         ...,\n",
       "         [ 2.31320076e-02,  2.87273177e-03,  1.69234793e-03],\n",
       "         [ 1.05445134e-03, -8.85738339e-03,  2.05029286e-02],\n",
       "         [-5.13769174e-03,  4.72501386e-03,  5.68725495e-03]],\n",
       "\n",
       "        [[ 2.45783129e-03,  5.39597997e-04, -3.47720715e-03],\n",
       "         [-5.37440239e-04, -1.20318856e-03,  5.46490715e-04],\n",
       "         [-1.26621956e-02, -9.87099702e-05,  1.86716020e-02],\n",
       "         ...,\n",
       "         [ 3.57456831e-03,  1.87431239e-02, -2.35487446e-02],\n",
       "         [ 4.19129431e-02, -5.73170930e-03,  1.11030564e-02],\n",
       "         [-2.82595940e-02, -3.05940229e-02,  5.03057148e-03]],\n",
       "\n",
       "        [[ 3.96103971e-03,  1.18934794e-03, -4.38705925e-03],\n",
       "         [ 2.33696750e-03, -1.38584282e-02, -1.73830788e-03],\n",
       "         [-3.57772619e-03, -9.71495453e-03, -1.24522438e-03],\n",
       "         ...,\n",
       "         [-4.39267745e-03,  6.52109273e-04,  1.87455993e-02],\n",
       "         [-1.04368972e-02, -6.63320348e-03, -1.97505895e-02],\n",
       "         [ 1.13159223e-02, -1.52703654e-02,  3.28047900e-03]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 1.52147969e-03, -4.14861785e-03, -5.75223099e-03],\n",
       "         [-8.07297882e-03, -7.19792303e-03,  1.74659304e-02],\n",
       "         [-1.72719341e-02,  1.06310928e-02, -2.12827064e-02],\n",
       "         ...,\n",
       "         [-4.33466062e-02, -8.36445106e-05, -3.14062042e-03],\n",
       "         [ 2.23260056e-02,  8.79895315e-03,  2.19785683e-02],\n",
       "         [-1.55560337e-02,  4.65839542e-03,  3.66170257e-02]],\n",
       "\n",
       "        [[-5.72489435e-03,  2.55657290e-03, -2.41620070e-03],\n",
       "         [ 3.79269291e-03, -7.38277985e-03,  9.56770126e-03],\n",
       "         [ 2.84376666e-02, -1.78454053e-02, -1.69245396e-02],\n",
       "         ...,\n",
       "         [-2.60766316e-02,  1.85695745e-03,  6.54491186e-02],\n",
       "         [ 2.90232450e-02, -3.70598100e-02,  3.58518190e-03],\n",
       "         [-8.80284701e-03, -1.47646135e-02,  5.54344710e-03]],\n",
       "\n",
       "        [[ 2.46813917e-03,  7.09974905e-03,  1.21149933e-02],\n",
       "         [ 7.14335311e-03,  1.15430206e-02,  2.33440343e-02],\n",
       "         [-9.96346213e-03,  8.66522733e-03, -1.04661277e-02],\n",
       "         ...,\n",
       "         [ 1.62176564e-02, -5.08822389e-02,  6.86049927e-03],\n",
       "         [ 1.63856298e-02, -1.79925002e-02, -3.88504169e-03],\n",
       "         [ 1.35530403e-03, -2.16753897e-03, -1.26563255e-02]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[ 4.58033988e-03,  2.05036998e-03,  1.82649959e-03],\n",
       "         [ 2.25102832e-03,  8.82178359e-03,  1.46696216e-03],\n",
       "         [ 7.18876999e-03, -6.41817693e-03,  5.19541884e-03],\n",
       "         ...,\n",
       "         [ 7.32604461e-03,  2.02545151e-03,  7.62463408e-03],\n",
       "         [-2.34047566e-02, -1.34790037e-02,  2.85274368e-02],\n",
       "         [ 4.53784689e-03, -1.04379319e-02,  1.12456502e-03]],\n",
       "\n",
       "        [[ 4.53750556e-03,  6.11169729e-03, -1.06723630e-03],\n",
       "         [-1.11322738e-02, -1.07908330e-03,  2.71078106e-03],\n",
       "         [-6.21391693e-04, -1.75742730e-02, -1.11311907e-03],\n",
       "         ...,\n",
       "         [-7.76790176e-03,  1.94907673e-02,  9.35287401e-03],\n",
       "         [ 3.43191344e-03, -1.88489780e-02,  1.65805407e-02],\n",
       "         [-4.48594149e-03,  9.21433605e-03,  6.80170581e-03]],\n",
       "\n",
       "        [[-2.08454626e-03, -1.39505742e-03,  1.32841039e-02],\n",
       "         [ 5.10224607e-03, -1.76970586e-02,  9.95215308e-03],\n",
       "         [-7.79325655e-03, -3.16315070e-02,  7.29998015e-03],\n",
       "         ...,\n",
       "         [-8.23273044e-03, -1.32144224e-02,  3.48226316e-02],\n",
       "         [-4.54511959e-03, -1.83954891e-02,  2.46509425e-02],\n",
       "         [ 6.04538573e-03, -1.31596364e-02,  6.48135273e-03]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 2.22482253e-02, -2.96849385e-03, -1.16087105e-02],\n",
       "         [ 2.36431994e-02, -3.66061274e-03,  2.40264684e-02],\n",
       "         [-1.27387070e-03, -2.40727663e-02,  8.05993658e-03],\n",
       "         ...,\n",
       "         [ 2.17103269e-02, -1.72289356e-03,  2.13653897e-03],\n",
       "         [ 5.24418708e-03, -3.09215635e-02,  4.31867689e-02],\n",
       "         [ 7.94511102e-03,  3.58113274e-03,  1.26339484e-03]],\n",
       "\n",
       "        [[ 9.39511508e-03, -6.73198083e-04,  1.29924869e-04],\n",
       "         [ 1.23564424e-02, -9.93866939e-03,  1.78455506e-02],\n",
       "         [-1.81261934e-02,  2.07141950e-03,  2.00673193e-02],\n",
       "         ...,\n",
       "         [ 2.73169251e-03,  1.22112259e-02, -2.77400832e-03],\n",
       "         [-2.26768176e-03, -1.69754103e-02,  1.38694998e-02],\n",
       "         [ 5.88057563e-03,  2.87805987e-03,  2.48723128e-03]],\n",
       "\n",
       "        [[-9.10807867e-03, -1.27403555e-03,  1.50816166e-03],\n",
       "         [ 1.63247585e-02, -2.70837103e-03, -2.25967728e-02],\n",
       "         [ 8.86455271e-03, -3.51535268e-02, -1.33828428e-02],\n",
       "         ...,\n",
       "         [-4.36010910e-03,  9.62122460e-04, -5.28986566e-03],\n",
       "         [-2.36001071e-02, -1.11457007e-02, -2.20112526e-03],\n",
       "         [-2.53279638e-02,  4.48015472e-03, -7.99030531e-03]]],\n",
       "\n",
       "\n",
       "       [[[ 3.30007658e-03, -4.22224961e-03,  3.14445491e-03],\n",
       "         [ 9.68872337e-04,  7.46899983e-03,  3.52810603e-03],\n",
       "         [ 3.95566691e-03, -9.27371555e-04,  1.49654020e-02],\n",
       "         ...,\n",
       "         [-4.12277551e-03, -1.39618106e-02, -5.24718687e-03],\n",
       "         [ 3.00377561e-03, -2.62404066e-02,  1.66173663e-03],\n",
       "         [-2.76745413e-03, -1.43322423e-02,  2.58596502e-02]],\n",
       "\n",
       "        [[-1.37073710e-03, -5.21791447e-03, -2.98608816e-03],\n",
       "         [-6.55602291e-03, -2.24543037e-03, -1.69212185e-02],\n",
       "         [ 8.93704419e-04, -2.49905139e-02, -1.90514699e-02],\n",
       "         ...,\n",
       "         [ 9.71795060e-03, -8.55540577e-03, -3.89129645e-03],\n",
       "         [ 1.46011347e-02, -3.75774801e-02, -2.21156236e-03],\n",
       "         [ 3.69514618e-03, -1.10154203e-03,  1.40111730e-03]],\n",
       "\n",
       "        [[-1.45241839e-03, -3.64956283e-03, -1.10374447e-02],\n",
       "         [ 3.58364843e-02, -1.37780327e-02,  1.65765099e-02],\n",
       "         [ 2.21243780e-03, -1.33385351e-02, -1.04853027e-02],\n",
       "         ...,\n",
       "         [-1.60804596e-02,  1.71123464e-02, -1.67308580e-02],\n",
       "         [-1.66292046e-03, -2.14449409e-02, -8.12065601e-03],\n",
       "         [ 1.24695571e-02, -1.35801015e-02,  1.16327563e-02]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 1.13853365e-02,  7.94237480e-03, -3.63238482e-03],\n",
       "         [-4.08773161e-02,  3.54339927e-02,  1.03800287e-02],\n",
       "         [-1.60615593e-02, -7.87398871e-03, -3.47165130e-02],\n",
       "         ...,\n",
       "         [ 2.64455993e-02,  2.27998253e-02,  1.74080487e-02],\n",
       "         [ 4.89494652e-02, -1.08042676e-02,  1.97222922e-02],\n",
       "         [ 7.48272985e-03, -1.27018131e-02,  7.66233448e-03]],\n",
       "\n",
       "        [[ 6.07984047e-03,  9.75554343e-03, -3.71981366e-03],\n",
       "         [-6.29752502e-03, -1.65060889e-02,  1.94689017e-02],\n",
       "         [ 2.66990103e-02, -1.01191569e-02,  8.08395911e-03],\n",
       "         ...,\n",
       "         [-2.03365944e-02, -5.81614636e-02,  2.59566698e-02],\n",
       "         [-3.38796489e-02, -2.54054572e-02,  3.72719369e-03],\n",
       "         [-1.73988175e-02, -1.77465975e-02,  6.67454442e-03]],\n",
       "\n",
       "        [[ 1.07586579e-02,  2.25455873e-03,  8.06515757e-03],\n",
       "         [ 1.68004837e-02,  3.65148261e-02, -4.15276131e-03],\n",
       "         [-2.23210827e-03,  1.61663967e-03, -3.45313586e-02],\n",
       "         ...,\n",
       "         [-4.10880474e-03, -2.08449531e-02,  4.60341526e-03],\n",
       "         [ 1.59404725e-02, -1.43768694e-02, -9.14168172e-03],\n",
       "         [-5.80202509e-03,  1.09892886e-03, -2.88140355e-03]]],\n",
       "\n",
       "\n",
       "       [[[ 5.14219655e-03, -5.79495914e-03,  3.83384828e-03],\n",
       "         [ 1.48892589e-02,  6.69672678e-04,  1.22384848e-02],\n",
       "         [-2.78221304e-03, -1.21605927e-02,  7.59960711e-03],\n",
       "         ...,\n",
       "         [ 1.94661673e-02,  2.24036654e-03, -2.23111957e-02],\n",
       "         [-2.08906215e-02, -5.22269495e-03,  1.20225223e-02],\n",
       "         [ 9.79785807e-03,  6.34382898e-03,  1.81155559e-03]],\n",
       "\n",
       "        [[-1.34646741e-03,  7.31467083e-03,  8.47275369e-04],\n",
       "         [-8.35814327e-03,  1.17989704e-02,  2.51553743e-03],\n",
       "         [-9.35436971e-03, -1.40312621e-02,  2.48550922e-02],\n",
       "         ...,\n",
       "         [-1.10641401e-02,  4.72822366e-03, -1.49877407e-02],\n",
       "         [ 1.74596272e-02, -1.80577710e-02,  4.99122683e-03],\n",
       "         [-5.51839732e-03,  5.38448337e-03,  8.50686990e-03]],\n",
       "\n",
       "        [[ 6.46829931e-03,  4.86530690e-03,  8.69179517e-03],\n",
       "         [ 1.22568682e-02, -1.03525314e-02,  1.31883854e-02],\n",
       "         [-3.30386334e-04, -1.40224618e-03,  2.36864780e-05],\n",
       "         ...,\n",
       "         [-4.96978965e-03, -3.26548778e-02,  1.42678926e-02],\n",
       "         [ 1.92940272e-02, -3.28130871e-02,  6.02220744e-03],\n",
       "         [-1.33772828e-02, -9.97169968e-03,  1.12446183e-02]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-6.28691260e-03, -1.24011689e-03, -8.06987006e-03],\n",
       "         [-1.57387578e-03, -2.22597434e-03, -2.55087968e-02],\n",
       "         [ 1.45897036e-02, -2.35763807e-02,  5.62034547e-03],\n",
       "         ...,\n",
       "         [ 4.42876928e-02,  5.99564053e-02, -3.27207632e-02],\n",
       "         [-1.04976185e-02, -5.25745749e-02,  3.83359492e-02],\n",
       "         [-6.27105497e-03, -9.27061774e-03,  1.98149141e-02]],\n",
       "\n",
       "        [[-9.47710965e-03, -7.07879663e-03,  4.51566186e-04],\n",
       "         [ 1.02188429e-02, -2.52464935e-02,  3.46835777e-02],\n",
       "         [ 2.67429855e-02, -5.89789962e-03, -1.79437753e-02],\n",
       "         ...,\n",
       "         [-1.56744123e-02,  1.06822615e-02,  1.89642310e-02],\n",
       "         [-2.80948542e-03, -3.23147252e-02, -4.96364385e-03],\n",
       "         [ 7.05139106e-03, -4.20343950e-02,  2.77609322e-02]],\n",
       "\n",
       "        [[-6.41341694e-03, -6.85502542e-03, -1.28373718e-02],\n",
       "         [ 3.55625674e-02,  3.73105332e-03,  2.49457564e-02],\n",
       "         [-1.03214206e-02,  7.19720963e-03, -1.54823484e-02],\n",
       "         ...,\n",
       "         [ 1.43609829e-02, -1.00060469e-02, -1.67596843e-02],\n",
       "         [-3.90181760e-03, -2.67953780e-02, -5.23525663e-03],\n",
       "         [-4.59566992e-03,  8.86249449e-03, -3.57041904e-03]]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.002"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_loader, model, output_activations=True):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for inputs, labels in data_loader:\n",
    "        if output_activations:\n",
    "            out, *_ = model(inputs, training=False)\n",
    "        else:\n",
    "            out = model(inputs, training=False)\n",
    "\n",
    "        prob = tf.math.softmax(out, axis=-1)\n",
    "        # prob = prob.numpy()\n",
    "\n",
    "        pred = tf.argmax(prob, axis=-1)\n",
    "        equality = tf.math.equal(pred.numpy(), tf.reshape(labels, [-1]).numpy())\n",
    "        correct += tf.reduce_sum(tf.cast(equality, tf.float32))\n",
    "        total += equality.shape[0]\n",
    "\n",
    "    ret = correct / tf.cast(total, tf.float32)\n",
    "    return ret.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.preprocess import get_cifar10_data\n",
    "(_, _), (x_test, y_test) = get_cifar10_data()\n",
    "\n",
    "test_data_loader = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.947"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(test_data_loader, teacher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
