{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from net.generator import *\n",
    "from utils.cosine_anealing import *\n",
    "from utils.losses import *\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from net.wide_resnet import WideResidualNetwork\n",
    "#from train_scratch import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_outputs(model, input):\n",
    "    \"\"\"\n",
    "    given model and the input data, outputs the logits and activations required for attention training\n",
    "\n",
    "    :param model: either student or teacher model\n",
    "    :param input: input batch of images\n",
    "    :param mode: 0 for test mode or 1 for train mode\n",
    "    :return: [logits, activations of 3 main blocks]\n",
    "\n",
    "    TODO:\n",
    "        1. use ```get_intm_outputs_of``` instead\n",
    "        2. Remove this when fully migrated\n",
    "    \"\"\"\n",
    "    output_layer_names = ['logits', 'attention1', 'attention2', 'attention3']\n",
    "    get_outputs = K.function([model.layers[0].input],\n",
    "                             [model.get_layer(l).output for l in output_layer_names])\n",
    "\n",
    "    return get_outputs([input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dim = 100\n",
    "batch_size = 128\n",
    "ng_batches = 1\n",
    "ns_batches = 10\n",
    "attn_beta = 250\n",
    "total_n_pseudo_batches = 10\n",
    "n_generator_items = ng_batches + ns_batches\n",
    "total_batches = 0\n",
    "student_lr = 2e-3\n",
    "generator_lr = 1e-3\n",
    "number_of_batches = 10\n",
    "\n",
    "teacher_model = WideResidualNetwork(16, 1, input_shape=(32, 32, 3), dropout_rate=0.0)\n",
    "teacher_model.load_weights('saved_models/cifar10_WRN-16-1_model.005.h5')\n",
    "\n",
    "student_model = WideResidualNetwork(16, 1, input_shape=(32, 32, 3), dropout_rate=0.0)\n",
    "student_optimizer=Adam(learning_rate=student_lr)\n",
    "student_scheduler = CosineAnnealingScheduler(T_max=number_of_batches, eta_max=student_lr, eta_min=0)\n",
    "\"\"\"\n",
    "compile student model with loss and lr_scheduler\n",
    "\"\"\"\n",
    "generator_model = generator(100)\n",
    "generator_optimizer=Adam(learning_rate=generator_lr)\n",
    "generator_scheduler = CosineAnnealingScheduler(T_max=number_of_batches, eta_max=generator_lr, eta_min=0)\n",
    "\"\"\"\n",
    "compile generator model with loss and lr_scheduler\n",
    "\"\"\"\n",
    "\n",
    "student_model.trainable = True\n",
    "teacher_model.trainable = False\n",
    "generator_model.trainable = True\n",
    "\n",
    "gen_loss_metric = tf.keras.metrics.Mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# algo 1\n",
    "loss_metric = tf.keras.metrics.Mean()\n",
    "\n",
    "for total_batches in range(total_n_pseudo_batches):\n",
    "    z = tf.random.normal([batch_size, z_dim])\n",
    "    pseudo_images = generator_model(z)\n",
    "    teacher_logits, *teacher_activations = get_model_outputs(teacher_model, pseudo_images, mode=0)\n",
    "    \n",
    "    #generator training\n",
    "    for ng in range(ng_batches):\n",
    "        with tf.GradientTape() as tape:\n",
    "            student_logits, *student_activations = get_model_outputs(student_model, pseudo_images, mode=1)\n",
    "            generator_loss = generator_loss(teacher_logits, student_logits)\n",
    "            \n",
    "        grads = tape.gradient(generator_loss, generator_model.trainable_weights)\n",
    "        generator_optimizer.apply_gradients(zip(grads, generator_model.trainable_weights))\n",
    "        \n",
    "        loss_metric(generator_loss)\n",
    "        \n",
    "        if total_batches % 100 == 0:\n",
    "            print('step %s: mean loss = %s' % (total_batches, loss_metric.result()))\n",
    "    \"\"\"   \n",
    "        #################\n",
    "        ### BACK PROP AND MODEL RELATED UPDATES\n",
    "        ##################\n",
    "        \n",
    "    for ns in range(ns_batches):\n",
    "        student_logits, *student_activations = get_model_outputs(student_model, pseudo_images, mode=1)\n",
    "        student_loss = student_loss(teacher_logits, teacher_activations, \n",
    "                                    student_logits, student_activations, attn_beta)\n",
    "        \n",
    "        #################\n",
    "        ### BACK PROP AND MODEL RELATED UPDATES\n",
    "        ##################    \n",
    "        \n",
    "    #Best test accuracy computation and best model saving\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: generator mean loss = tf.Tensor(1.1801524, shape=(), dtype=float32)\n",
      "step 2: generator mean loss = tf.Tensor(1.0724399, shape=(), dtype=float32)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-903798a347e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[0mgenerator_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKLD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mteacher_logits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstudent_logits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m             \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenerator_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m             \u001b[0mgenerator_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenerator_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1012\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[0;32m   1015\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     74\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\ops\\nn_grad.py\u001b[0m in \u001b[0;36m_Conv2DGrad\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m    604\u001b[0m           \u001b[0mexplicit_paddings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexplicit_paddings\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m           \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m           data_format=data_format)\n\u001b[0m\u001b[0;32m    607\u001b[0m   ]\n\u001b[0;32m    608\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d_backprop_filter\u001b[1;34m(input, filter_sizes, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[0;32m   1184\u001b[0m         \u001b[0mfilter_sizes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_backprop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"strides\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"use_cudnn_on_gpu\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m         \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"padding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"explicit_paddings\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1186\u001b[1;33m         explicit_paddings, \"data_format\", data_format, \"dilations\", dilations)\n\u001b[0m\u001b[0;32m   1187\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for total_batches in range(total_n_pseudo_batches):\n",
    "    with tf.GradientTape() as tape:\n",
    "        z = tf.random.normal([batch_size, z_dim])\n",
    "        pseudo_images = generator_model(z)\n",
    "        #teacher_logits, *teacher_activations = get(teacher_model, pseudo_images, mode=0)\n",
    "        teacher_logits = teacher_model(pseudo_images)\n",
    "\n",
    "        #generator training\n",
    "        for ng in range(ng_batches):\n",
    "        \n",
    "            #pseudo_images = generator_model(z)\n",
    "            student_logits = student_model(pseudo_images)\n",
    "            #teacher_logits = teacher_model(pseudo_images)\n",
    "            #teacher_logits, *teacher_activations = get_model_outputs(teacher_model, pseudo_images)\n",
    "            #act1 = teacher_model\n",
    "            # teacher_logits, *teacher_activations = get_intm_outputs_of(teacher_model, pseudo_images, \"eval\")\n",
    "            generator_loss = tf.keras.metrics.KLD(teacher_logits, student_logits)\n",
    "\n",
    "            grads = tape.gradient(generator_loss, generator_model.trainable_weights)\n",
    "            generator_optimizer.apply_gradients(zip(grads, generator_model.trainable_weights))\n",
    "\n",
    "            gen_loss_metric(generator_loss)\n",
    "\n",
    "            if total_batches % 2 == 0:\n",
    "                print('step %s: generator mean loss = %s' % (total_batches, gen_loss_metric.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
