{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ZeroShot (Algo 1)\n",
    "\n",
    "    for total_batches in range(total_n_pseudo_batches):\n",
    "    \n",
    "        z = tf.random.normal([batch_size, z_dim])\n",
    "        pseudo_images = get_gen_images(z)\n",
    "        teacher_logits, *teacher_activations = get_model_outputs(teacher_model, pseudo_images, mode=0)\n",
    "\n",
    "        #generator training\n",
    "        for ng in range(ng_batches):\n",
    "            student_logits, *student_activations = get_model_outputs(student_model, pseudo_images, mode=1)\n",
    "            generator_loss = generator_loss(teacher_logits, student_logits)\n",
    "\n",
    "            #################################\n",
    "            # BACK PROP AND tick schedulers #\n",
    "            #################################  \n",
    "\n",
    "        for ns in range(ns_batches):\n",
    "            student_logits, *student_activations = get_model_outputs(student_model, pseudo_images, mode=1)\n",
    "            student_loss = student_loss(teacher_logits, teacher_activations, \n",
    "                                        student_logits, student_activations, attn_beta)\n",
    "\n",
    "            #################################\n",
    "            # BACK PROP AND tick schedulers #\n",
    "            #################################   \n",
    "\n",
    "        ######################################################\n",
    "        ### Val accuracy computation and best model saving ###\n",
    "        ######################################################    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from net.generator import NavieGenerator\n",
    "from utils.cosine_anealing import CosineAnnealingScheduler\n",
    "from utils.losses import *\n",
    "#from utils.losses import student_loss_fn\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from net.wide_resnet import WideResidualNetwork\n",
    "from tensorflow.keras.experimental import CosineDecay\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dim = 100\n",
    "batch_size = 128\n",
    "ng_batches = 1\n",
    "ns_batches = 5\n",
    "attn_beta = 250\n",
    "total_n_pseudo_batches = 3\n",
    "n_generator_items = ng_batches + ns_batches\n",
    "student_lr = 2e-3\n",
    "generator_lr = 1e-3\n",
    "number_of_batches = 3\n",
    "\n",
    "teacher = WideResidualNetwork(16, 1, input_shape=(32, 32, 3), dropout_rate=0.0, output_activations=True)\n",
    "teacher.load_weights('saved_models/cifar10_WRN-16-1_model.005.h5')\n",
    "teacher.trainable = False\n",
    "\n",
    "student = WideResidualNetwork(16, 1, input_shape=(32, 32, 3), dropout_rate=0.0, output_activations=True)\n",
    "student_optimizer = Adam(learning_rate=CosineDecay(student_lr, number_of_batches))\n",
    "\n",
    "generator = NavieGenerator(input_dim=100)\n",
    "generator_optimizer = Adam(learning_rate=CosineDecay(generator_lr, number_of_batches))\n",
    "\n",
    "# Generator loss metrics\n",
    "g_loss_met = tf.keras.metrics.Mean()\n",
    "# Student loss metrics\n",
    "stu_loss_met = tf.keras.metrics.Mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: generator mean loss = -1.5264208\n",
      "step 0-0: studnt mean loss = 1.5345669\n",
      "step 0-1: studnt mean loss = 1.4271271\n",
      "step 0-2: studnt mean loss = 1.2087471\n",
      "step 0-3: studnt mean loss = 1.0335943\n",
      "step 0-4: studnt mean loss = 0.92850244\n",
      "step 1: generator mean loss = -0.92834574\n",
      "step 1-0: studnt mean loss = 0.8294286\n",
      "step 1-1: studnt mean loss = 0.75866157\n",
      "step 1-2: studnt mean loss = 0.70558625\n",
      "step 1-3: studnt mean loss = 0.66430545\n",
      "step 1-4: studnt mean loss = 0.63128084\n",
      "step 2: generator mean loss = -0.70232373\n",
      "step 2-0: studnt mean loss = 0.59703016\n",
      "step 2-1: studnt mean loss = 0.56848794\n",
      "step 2-2: studnt mean loss = 0.5443368\n",
      "step 2-3: studnt mean loss = 0.5236358\n",
      "step 2-4: studnt mean loss = 0.505695\n"
     ]
    }
   ],
   "source": [
    "for total_batches in range(total_n_pseudo_batches):\n",
    "    # sample from latern space to make an image\n",
    "    z = tf.random.normal([batch_size, z_dim])\n",
    "\n",
    "    # Generator training\n",
    "    generator.trainable = True\n",
    "    student.trainable = False\n",
    "    for ng in range(ng_batches):\n",
    "        with tf.GradientTape() as tape:\n",
    "            pseudo_imgs = generator(z)\n",
    "            t_logits, *_ = teacher(pseudo_imgs)\n",
    "            s_logits, *_ = student(pseudo_imgs)\n",
    "\n",
    "            # calculate the generator loss\n",
    "            loss = generator_loss_fn(t_logits, s_logits)\n",
    "\n",
    "        # The grad for generator\n",
    "        grads = tape.gradient(loss, generator.trainable_weights)\n",
    "\n",
    "        # update the generator paramter with the gradient\n",
    "        generator_optimizer.apply_gradients(zip(grads, generator.trainable_weights))\n",
    "\n",
    "        g_loss_met(loss)\n",
    "\n",
    "        print('step %s: generator mean loss = %s' % (total_batches, g_loss_met.result().numpy()))\n",
    "    # ==========================================================================\n",
    "\n",
    "    # Student training\n",
    "    generator.trainable = False\n",
    "    student.trainable = True\n",
    "    for ns in range(ns_batches):\n",
    "\n",
    "        t_logits, *t_acts = teacher(pseudo_imgs)\n",
    "        with tf.GradientTape() as tape:\n",
    "            s_logits, *s_acts = student(pseudo_imgs)\n",
    "            loss = student_loss_fn(t_logits, t_acts, s_logits, s_acts, attn_beta)\n",
    "\n",
    "        # The grad for student\n",
    "        grads = tape.gradient(loss, student.trainable_weights)\n",
    "\n",
    "        # Apply grad for student\n",
    "        student_optimizer.apply_gradients(zip(grads, student.trainable_weights))\n",
    "\n",
    "        stu_loss_met(loss)\n",
    "\n",
    "        print('step %s-%s: studnt mean loss = %s' % (total_batches, ns, stu_loss_met.result().numpy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.preprocess import load_cifar10_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = load_cifar10_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(student):\n",
    "    model = WideResidualNetwork(16, 1, input_shape=(32, 32, 3), dropout_rate=0.0)\n",
    "    model.set_weights(student.get_weights()) \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "    loss, accuracy = model.evaluate(x_test, y_test, batch_size=128, verbose=0)\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "call() got an unexpected keyword argument 'batch_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-962a266ee134>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mstest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstudent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[0;32m    890\u001b[0m               self._compute_dtype):\n\u001b[1;32m--> 891\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: call() got an unexpected keyword argument 'batch_size'"
     ]
    }
   ],
   "source": [
    "stest = student(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.66 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(29.924496099853517, 0.1)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "get_accuracy(student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = True\n",
    "b = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "if a and not b:\n",
    "    print(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 577 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = WideResidualNetwork(16, 1, input_shape=(32, 32, 3), dropout_rate=0.0)\n",
    "model.set_weights(student.get_weights()) \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 2.3115152214050294\n",
      "Test accuracy: 0.1098\n",
      "Wall time: 7.44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
