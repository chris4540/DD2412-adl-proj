{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from net.generator import *\n",
    "from utils.cosine_anealing import *\n",
    "#from utils.losses import *\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from net.wide_resnet import WideResidualNetwork\n",
    "from train_scratch import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dim = 100\n",
    "batch_size = 128\n",
    "ng_batches = 1\n",
    "ns_batches = 10\n",
    "attn_beta = 250\n",
    "total_n_pseudo_batches = 10\n",
    "n_generator_items = ng_batches + ns_batches\n",
    "total_batches = 0\n",
    "student_lr = 2e-3\n",
    "generator_lr = 1e-3\n",
    "number_of_batches = 10\n",
    "\n",
    "teacher_model = WideResidualNetwork(16, 1, input_shape=(32, 32, 3), dropout_rate=0.0)\n",
    "teacher_model.load_weights('saved_models/cifar10_WRN-16-1_model.005.h5')\n",
    "\n",
    "student_model = WideResidualNetwork(16, 1, input_shape=(32, 32, 3), dropout_rate=0.0)\n",
    "student_optimizer=Adam(learning_rate=student_lr)\n",
    "student_scheduler = CosineAnnealingScheduler(T_max=number_of_batches, eta_max=student_lr, eta_min=0)\n",
    "\n",
    "generator_model = generator(100)\n",
    "generator_optimizer=Adam(learning_rate=generator_lr)\n",
    "generator_scheduler = CosineAnnealingScheduler(T_max=number_of_batches, eta_max=generator_lr, eta_min=0)\n",
    "\n",
    "student_model.trainable = True\n",
    "teacher_model.trainable = False\n",
    "generator_model.trainable = True\n",
    "\n",
    "gen_loss_metric = tf.keras.metrics.Mean()\n",
    "stu_loss_metric = tf.keras.metrics.Mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# algo 1\n",
    "\n",
    "    for total_batches in range(total_n_pseudo_batches):\n",
    "    \n",
    "        z = tf.random.normal([batch_size, z_dim])\n",
    "        pseudo_images = get_gen_images(z)\n",
    "        teacher_logits, *teacher_activations = get_model_outputs(teacher_model, pseudo_images, mode=0)\n",
    "\n",
    "        #generator training\n",
    "        for ng in range(ng_batches):\n",
    "            student_logits, *student_activations = get_model_outputs(student_model, pseudo_images, mode=1)\n",
    "            generator_loss = generator_loss(teacher_logits, student_logits)\n",
    "\n",
    "            #################################\n",
    "            # BACK PROP AND tick schedulers #\n",
    "            #################################  \n",
    "\n",
    "        for ns in range(ns_batches):\n",
    "            student_logits, *student_activations = get_model_outputs(student_model, pseudo_images, mode=1)\n",
    "            student_loss = student_loss(teacher_logits, teacher_activations, \n",
    "                                        student_logits, student_activations, attn_beta)\n",
    "\n",
    "            #################################\n",
    "            # BACK PROP AND tick schedulers #\n",
    "            #################################   \n",
    "\n",
    "        ######################################################\n",
    "        ### Val accuracy computation and best model saving ###\n",
    "        ######################################################    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer_names = ['logits', 'attention1', 'attention2', 'attention3']\n",
    "t_model = Model(teacher_model.input, [teacher_model.get_layer(l).output for l in output_layer_names])\n",
    "s_model = Model(student_model.input, [student_model.get_layer(l).output for l in output_layer_names])\n",
    "\n",
    "def cosine_lr_schedule(epoch, T_max, eta_max, eta_min=0):\n",
    "    lr = eta_min + (eta_max - eta_min) * (1 + math.cos(math.pi * epoch / T_max)) / 2\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: generator mean loss = tf.Tensor(1.0912918, shape=(), dtype=float32)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable: ['conv2d_15/kernel:0', 'batch_normalization_13/gamma:0', 'batch_normalization_13/beta:0', 'conv2d_16/kernel:0', 'batch_normalization_14/gamma:0', 'batch_normalization_14/beta:0', 'conv2d_17/kernel:0', 'batch_normalization_15/gamma:0', 'batch_normalization_15/beta:0', 'conv2d_18/kernel:0', 'batch_normalization_16/gamma:0', 'batch_normalization_16/beta:0', 'conv2d_19/kernel:0', 'batch_normalization_17/gamma:0', 'batch_normalization_17/beta:0', 'conv2d_20/kernel:0', 'batch_normalization_18/gamma:0', 'batch_normalization_18/beta:0', 'conv2d_22/kernel:0', 'conv2d_21/kernel:0', 'batch_normalization_19/gamma:0', 'batch_normalization_19/beta:0', 'conv2d_23/kernel:0', 'batch_normalization_20/gamma:0', 'batch_normalization_20/beta:0', 'conv2d_24/kernel:0', 'batch_normalization_21/gamma:0', 'batch_normalization_21/beta:0', 'conv2d_25/kernel:0', 'batch_normalization_22/gamma:0', 'batch_normalization_22/beta:0', 'conv2d_27/kernel:0', 'conv2d_26/kernel:0', 'batch_normalization_23/gamma:0', 'batch_normalization_23/beta:0', 'conv2d_28/kernel:0', 'batch_normalization_24/gamma:0', 'batch_normalization_24/beta:0', 'conv2d_29/kernel:0', 'batch_normalization_25/gamma:0', 'batch_normalization_25/beta:0', 'logits_1/kernel:0', 'logits_1/bias:0'].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-021fe6690989>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mstudent_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcosine_lr_schedule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_batches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_n_pseudo_batches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstudent_lr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0mstudent_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mst_grads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mstu_loss_metric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstudent_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[1;34m(self, grads_and_vars, name)\u001b[0m\n\u001b[0;32m    425\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mnone\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mvariables\u001b[0m \u001b[0mhave\u001b[0m \u001b[0mgradients\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m     \"\"\"\n\u001b[1;32m--> 427\u001b[1;33m     \u001b[0mgrads_and_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_filter_grads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    428\u001b[0m     \u001b[0mvar_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36m_filter_grads\u001b[1;34m(grads_and_vars)\u001b[0m\n\u001b[0;32m   1023\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfiltered\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m     raise ValueError(\"No gradients provided for any variable: %s.\" %\n\u001b[1;32m-> 1025\u001b[1;33m                      ([v.name for _, v in grads_and_vars],))\n\u001b[0m\u001b[0;32m   1026\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mvars_with_empty_grads\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1027\u001b[0m     logging.warning(\n",
      "\u001b[1;31mValueError\u001b[0m: No gradients provided for any variable: ['conv2d_15/kernel:0', 'batch_normalization_13/gamma:0', 'batch_normalization_13/beta:0', 'conv2d_16/kernel:0', 'batch_normalization_14/gamma:0', 'batch_normalization_14/beta:0', 'conv2d_17/kernel:0', 'batch_normalization_15/gamma:0', 'batch_normalization_15/beta:0', 'conv2d_18/kernel:0', 'batch_normalization_16/gamma:0', 'batch_normalization_16/beta:0', 'conv2d_19/kernel:0', 'batch_normalization_17/gamma:0', 'batch_normalization_17/beta:0', 'conv2d_20/kernel:0', 'batch_normalization_18/gamma:0', 'batch_normalization_18/beta:0', 'conv2d_22/kernel:0', 'conv2d_21/kernel:0', 'batch_normalization_19/gamma:0', 'batch_normalization_19/beta:0', 'conv2d_23/kernel:0', 'batch_normalization_20/gamma:0', 'batch_normalization_20/beta:0', 'conv2d_24/kernel:0', 'batch_normalization_21/gamma:0', 'batch_normalization_21/beta:0', 'conv2d_25/kernel:0', 'batch_normalization_22/gamma:0', 'batch_normalization_22/beta:0', 'conv2d_27/kernel:0', 'conv2d_26/kernel:0', 'batch_normalization_23/gamma:0', 'batch_normalization_23/beta:0', 'conv2d_28/kernel:0', 'batch_normalization_24/gamma:0', 'batch_normalization_24/beta:0', 'conv2d_29/kernel:0', 'batch_normalization_25/gamma:0', 'batch_normalization_25/beta:0', 'logits_1/kernel:0', 'logits_1/bias:0']."
     ]
    }
   ],
   "source": [
    "for total_batches in range(total_n_pseudo_batches):\n",
    "    z = tf.random.normal([batch_size, z_dim])\n",
    "    #pseudo_images = generator_model(z)\n",
    "    #teacher_logits, *teacher_activations = get(teacher_model, pseudo_images, mode=0)\n",
    "    #teacher_logits, *teacher_activations = t_model(pseudo_images)\n",
    "\n",
    "    #generator training\n",
    "    for ng in range(ng_batches):\n",
    "        with tf.GradientTape() as gtape:\n",
    "            pseudo_images = generator_model(z)\n",
    "            teacher_logits, *teacher_activations = t_model(pseudo_images)\n",
    "            student_logits, *student_activations = s_model(pseudo_images)\n",
    "            generator_loss = kd_loss(tf.math.softmax(teacher_logits), tf.math.softmax(student_logits))\n",
    "\n",
    "        gen_grads = gtape.gradient(generator_loss, generator_model.trainable_weights)\n",
    "        \n",
    "        #cosine annealing for learning rate\n",
    "        generator_optimizer.learning_rate = cosine_lr_schedule(total_batches, total_n_pseudo_batches, generator_lr)\n",
    "        \n",
    "        #update gradient\n",
    "        generator_optimizer.apply_gradients(zip(gen_grads, generator_model.trainable_weights))\n",
    "\n",
    "        gen_loss_metric(generator_loss)\n",
    "\n",
    "        if total_batches % 2 == 0:\n",
    "            print('step %s: generator mean loss = %s' % (total_batches, gen_loss_metric.result()))\n",
    "    \n",
    "    for ns in range(2):\n",
    "        with tf.GradientTape() as stape:\n",
    "            pseudo_images = generator_model(z)\n",
    "            teacher_logits, *teacher_activations = t_model(pseudo_images)\n",
    "            student_logits, *student_activations = s_model(pseudo_images)\n",
    "            student_loss = student_loss(teacher_logits, teacher_activations, \n",
    "                                student_logits, student_activations)\n",
    "\n",
    "        st_grads = stape.gradient(generator_loss, s_model.trainable_weights)\n",
    "        \n",
    "        student_optimizer.learning_rate = cosine_lr_schedule(total_batches, total_n_pseudo_batches, student_lr)\n",
    "        student_optimizer.apply_gradients(zip(st_grads, s_model.trainable_weights))\n",
    "\n",
    "        stu_loss_metric(student_loss)\n",
    "\n",
    "        if total_batches % 2 == 0:\n",
    "            print('step %s: studnt mean loss = %s' % (total_batches, stu_loss_metric.result()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
