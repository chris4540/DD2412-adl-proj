WideResNet(
  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (block1): NetworkBlock(
    (layer): Sequential(
      (0): BasicBlock(
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (1): BasicBlock(
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (block2): NetworkBlock(
    (layer): Sequential(
      (0): BasicBlock(
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (convShortcut): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      )
      (1): BasicBlock(
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (block3): NetworkBlock(
    (layer): Sequential(
      (0): BasicBlock(
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (convShortcut): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      )
      (1): BasicBlock(
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
-----------------------------------
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 16, 32, 32]             432
       BatchNorm2d-2           [-1, 16, 32, 32]              32
              ReLU-3           [-1, 16, 32, 32]               0
            Conv2d-4           [-1, 16, 32, 32]           2,304
       BatchNorm2d-5           [-1, 16, 32, 32]              32
              ReLU-6           [-1, 16, 32, 32]               0
            Conv2d-7           [-1, 16, 32, 32]           2,304
        BasicBlock-8           [-1, 16, 32, 32]               0
       BatchNorm2d-9           [-1, 16, 32, 32]              32
             ReLU-10           [-1, 16, 32, 32]               0
           Conv2d-11           [-1, 16, 32, 32]           2,304
      BatchNorm2d-12           [-1, 16, 32, 32]              32
             ReLU-13           [-1, 16, 32, 32]               0
           Conv2d-14           [-1, 16, 32, 32]           2,304
       BasicBlock-15           [-1, 16, 32, 32]               0
     NetworkBlock-16           [-1, 16, 32, 32]               0
      BatchNorm2d-17           [-1, 16, 32, 32]              32
             ReLU-18           [-1, 16, 32, 32]               0
           Conv2d-19           [-1, 32, 16, 16]           4,608
      BatchNorm2d-20           [-1, 32, 16, 16]              64
             ReLU-21           [-1, 32, 16, 16]               0
           Conv2d-22           [-1, 32, 16, 16]           9,216
           Conv2d-23           [-1, 32, 16, 16]             512
       BasicBlock-24           [-1, 32, 16, 16]               0
      BatchNorm2d-25           [-1, 32, 16, 16]              64
             ReLU-26           [-1, 32, 16, 16]               0
           Conv2d-27           [-1, 32, 16, 16]           9,216
      BatchNorm2d-28           [-1, 32, 16, 16]              64
             ReLU-29           [-1, 32, 16, 16]               0
           Conv2d-30           [-1, 32, 16, 16]           9,216
       BasicBlock-31           [-1, 32, 16, 16]               0
     NetworkBlock-32           [-1, 32, 16, 16]               0
      BatchNorm2d-33           [-1, 32, 16, 16]              64
             ReLU-34           [-1, 32, 16, 16]               0
           Conv2d-35             [-1, 64, 8, 8]          18,432
      BatchNorm2d-36             [-1, 64, 8, 8]             128
             ReLU-37             [-1, 64, 8, 8]               0
           Conv2d-38             [-1, 64, 8, 8]          36,864
           Conv2d-39             [-1, 64, 8, 8]           2,048
       BasicBlock-40             [-1, 64, 8, 8]               0
      BatchNorm2d-41             [-1, 64, 8, 8]             128
             ReLU-42             [-1, 64, 8, 8]               0
           Conv2d-43             [-1, 64, 8, 8]          36,864
      BatchNorm2d-44             [-1, 64, 8, 8]             128
             ReLU-45             [-1, 64, 8, 8]               0
           Conv2d-46             [-1, 64, 8, 8]          36,864
       BasicBlock-47             [-1, 64, 8, 8]               0
     NetworkBlock-48             [-1, 64, 8, 8]               0
      BatchNorm2d-49             [-1, 64, 8, 8]             128
             ReLU-50             [-1, 64, 8, 8]               0
           Linear-51                   [-1, 10]             650
================================================================
Total params: 175,066
Trainable params: 175,066
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 3.75
Params size (MB): 0.67
Estimated Total Size (MB): 4.43
----------------------------------------------------------------
None
